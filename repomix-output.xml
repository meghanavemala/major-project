This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.gitignore
app.py
CONTRIBUTING.md
docker-compose.yml
Dockerfile
processed/.gitkeep
README.md
requirements.txt
SETUP.md
static/script.js
static/style.css
templates/index.html
uploads/.gitkeep
utils/clustering.py
utils/downloader.py
utils/keyframes.py
utils/summarizer.py
utils/transcriber.py
utils/translator.py
utils/tts.py
utils/video_maker.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="CONTRIBUTING.md">
# Contributing to Video Summarization Web App

Thank you for your interest in contributing! Here's how you can help improve this project.

## üöÄ Getting Started

1. **Fork** the repository on GitHub
2. **Clone** your forked repository
   ```bash
   git clone https://github.com/your-username/video-summarization-app.git
   cd video-summarization-app
   ```
3. **Set up** the development environment:
   ```bash
   # Create and activate virtual environment (Windows)
   python -m venv venv
   .\venv\Scripts\activate
   
   # Install dependencies
   pip install -r requirements-dev.txt
   
   # Install pre-commit hooks
   pre-commit install
   ```
4. **Run tests** to verify your setup:
   ```bash
   python -m pytest
   ```

## üõ† Development Workflow

1. Create a new branch for your feature or bugfix:
   ```bash
   git checkout -b feature/your-feature-name
   # or
   git checkout -b bugfix/issue-number-description
   ```

2. Make your changes following the code style guidelines

3. Write or update tests as needed

4. Run tests and fix any issues:
   ```bash
   python -m pytest
   ```

5. Commit your changes with a descriptive message:
   ```bash
   git add .
   git commit -m "feat: add new feature"
   # or
   git commit -m "fix: resolve issue with video processing"
   ```

6. Push your changes to your fork:
   ```bash
   git push origin your-branch-name
   ```

7. Open a **Pull Request** against the `main` branch

## üìù Code Style

- Follow [PEP 8](https://www.python.org/dev/peps/pep-0008/) for Python code
- Use type hints for function signatures
- Write docstrings for all public functions and classes
- Keep lines under 88 characters (Black's default line length)
- Use meaningful variable and function names

## üß™ Testing

- Write unit tests for new features and bug fixes
- Ensure all tests pass before submitting a PR
- Use descriptive test function names that describe the behavior being tested
- Follow the Arrange-Act-Assert pattern in tests

## üìö Documentation

- Update the README.md for significant changes
- Add docstrings to new functions and classes
- Document any new environment variables or configuration options

## üêõ Reporting Issues

When reporting issues, please include:

1. A clear, descriptive title
2. Steps to reproduce the issue
3. Expected vs. actual behavior
4. Environment details (OS, Python version, etc.)
5. Any relevant error messages or logs

## ü§ù Code of Conduct

Please note that this project is released with a [Contributor Code of Conduct](CODE_OF_CONDUCT.md). By participating in this project you agree to abide by its terms.

## üìú License

By contributing, you agree that your contributions will be licensed under the project's [LICENSE](LICENSE) file.

## üôè Thank You!

Your contributions make open-source a fantastic place to learn, inspire, and create. Thank you for being part of our community!
</file>

<file path="docker-compose.yml">
version: '3.8'

services:
  app:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - ./uploads:/app/uploads
      - ./processed:/app/processed
    environment:
      - FLASK_APP=app.py
      - FLASK_ENV=production
      - FLASK_SECRET_KEY=${FLASK_SECRET_KEY:-dev-key-change-in-production}
      - MAX_CONTENT_LENGTH=41943040
      - MAX_VIDEO_DURATION=2400
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s

  # Optional: Add Redis for caching and background task queue in the future
  # redis:
  #   image: redis:alpine
  #   ports:
  #     - "6379:6379"
  #   volumes:
  #     - redis_data:/data
  #   restart: unless-stopped

# volumes:
#   redis_data:
</file>

<file path="Dockerfile">
# Use Python 3.9 slim as the base image
FROM python:3.9-slim

# Set environment variables
ENV PYTHONDONTWRITEBYTECODE 1
ENV PYTHONUNBUFFERED 1
ENV FLASK_APP=app.py
ENV FLASK_ENV=production

# Install system dependencies
RUN apt-get update && apt-get install -y \
    ffmpeg \
    libsm6 \
    libxext6 \
    && rm -rf /var/lib/apt/lists/*

# Set the working directory
WORKDIR /app

# Copy requirements first to leverage Docker cache
COPY requirements.txt .

# Install Python dependencies
RUN pip install --no-cache-dir -r requirements.txt

# Download NLTK data
RUN python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"

# Copy the rest of the application code
COPY . .

# Create necessary directories
RUN mkdir -p uploads processed

# Expose the port the app runs on
EXPOSE 5000

# Command to run the application
CMD ["gunicorn", "--bind", "0.0.0.0:5000", "app:app"]
</file>

<file path="processed/.gitkeep">
# This file ensures the processed directory is tracked by git
</file>

<file path="static/script.js">
/**
 * Enhanced Video Summarizer Frontend JavaScript
 * 
 * This script handles the complete frontend functionality for the AI Video Summarizer,
 * including form submission, progress tracking, topic selection, and result display.
 * 
 * Features:
 * - Real-time progress tracking with step indicators
 * - Dynamic topic selection interface
 * - Video player for topic summaries
 * - Language-aware interface updates
 * - Download functionality
 * - Error handling and user feedback
 * 
 * Author: Video Summarizer Team
 * Created: 2024
 */

// Global state management
let currentVideoId = null;
let processingInterval = null;
let summaryData = null;

// DOM element references
const elements = {
    form: document.getElementById('main-form'),
    loader: document.getElementById('loader'),
    resultContainer: document.getElementById('result-container'),
    progressPercentage: document.getElementById('progress-percentage'),
    progressFill: document.getElementById('progress-fill'),
    processingTitle: document.getElementById('processing-title'),
    processingMessage: document.getElementById('processing-message'),
    topicsGrid: document.getElementById('topics-grid'),
    topicPlayer: document.getElementById('topic-player'),
    topicVideo: document.getElementById('topic-video'),
    currentTopicTitle: document.getElementById('current-topic-title'),
    topicSummaryText: document.getElementById('topic-summary-text'),
    topicKeywordsList: document.getElementById('topic-keywords-list'),
    closePlayerBtn: document.getElementById('close-player'),
    
    // Form elements
    sourceLanguage: document.getElementById('source-language'),
    targetLanguage: document.getElementById('target-language'),
    voice: document.getElementById('voice'),
    resolution: document.getElementById('resolution'),
    summaryLength: document.getElementById('summary_length'),
    enableOcr: document.getElementById('enable_ocr'),
    videoUpload: document.getElementById('video-upload'),
    ytUrl: document.getElementById('yt-url'),
    
    // Processing steps
    stepDownload: document.getElementById('step-download'),
    stepTranscribe: document.getElementById('step-transcribe'),
    stepKeyframes: document.getElementById('step-keyframes'),
    stepAnalyze: document.getElementById('step-analyze'),
    stepGenerate: document.getElementById('step-generate'),
};

// Processing step configuration
const PROCESSING_STEPS = {
    'downloading': { element: elements.stepDownload, message: 'Downloading video...', progress: [0, 10] },
    'transcribing': { element: elements.stepTranscribe, message: 'Converting speech to text...', progress: [10, 30] },
    'extracting': { element: elements.stepKeyframes, message: 'Extracting key frames...', progress: [30, 50] },
    'clustering': { element: elements.stepAnalyze, message: 'Analyzing content and topics...', progress: [50, 70] },
    'summarizing': { element: elements.stepAnalyze, message: 'Generating summaries...', progress: [70, 85] },
    'rendering': { element: elements.stepGenerate, message: 'Creating summary videos...', progress: [85, 95] },
    'completed': { element: null, message: 'Processing complete!', progress: [95, 100] }
};

/**
 * Initialize the application when DOM is loaded
 */
document.addEventListener('DOMContentLoaded', function() {
    initializeEventListeners();
    initializeLanguageHandlers();
    loadUserPreferences();
});

/**
 * Set up all event listeners for the application
 */
function initializeEventListeners() {
    // Form submission
    elements.form.addEventListener('submit', handleFormSubmission);
    
    // File upload handling
    elements.videoUpload.addEventListener('change', handleFileSelection);
    
    // Language change handlers
    elements.targetLanguage.addEventListener('change', updateVoiceOptions);
    
    // Player controls
    elements.closePlayerBtn.addEventListener('click', closeTopicPlayer);
    
    // Download buttons
    document.getElementById('download-all')?.addEventListener('click', downloadAllSummaries);
    document.getElementById('download-transcript')?.addEventListener('click', downloadTranscript);
    document.getElementById('download-keyframes')?.addEventListener('click', downloadKeyframes);
    
    // Keyboard shortcuts
    document.addEventListener('keydown', handleKeyboardShortcuts);
}

/**
 * Initialize language-specific handlers
 */
function initializeLanguageHandlers() {
    // Auto-update voice options when target language changes
    updateVoiceOptions();
    
    // Handle auto-detect language changes
    elements.sourceLanguage.addEventListener('change', function() {
        if (this.value === 'auto') {
            showTooltip(this, 'Language will be automatically detected during processing');
        }
    });
}

/**
 * Load user preferences from localStorage
 */
function loadUserPreferences() {
    const preferences = JSON.parse(localStorage.getItem('videoSummarizerPrefs') || '{}');
    
    if (preferences.targetLanguage) {
        elements.targetLanguage.value = preferences.targetLanguage;
    }
    if (preferences.voice) {
        elements.voice.value = preferences.voice;
    }
    if (preferences.resolution) {
        elements.resolution.value = preferences.resolution;
    }
    if (preferences.summaryLength) {
        elements.summaryLength.value = preferences.summaryLength;
    }
    if (preferences.enableOcr !== undefined) {
        elements.enableOcr.checked = preferences.enableOcr;
    }
    
    updateVoiceOptions();
}

/**
 * Save user preferences to localStorage
 */
function saveUserPreferences() {
    const preferences = {
        targetLanguage: elements.targetLanguage.value,
        voice: elements.voice.value,
        resolution: elements.resolution.value,
        summaryLength: elements.summaryLength.value,
        enableOcr: elements.enableOcr.checked
    };
    
    localStorage.setItem('videoSummarizerPrefs', JSON.stringify(preferences));
}

/**
 * Handle form submission for video processing
 */
async function handleFormSubmission(e) {
    e.preventDefault();

    // Validate form
    if (!validateForm()) {
        return;
    }
    
    // Save user preferences
    saveUserPreferences();
    
    // Prepare form data
    const formData = new FormData(elements.form);
    
    // Show processing UI
    showProcessingUI();
    
    try {
        // Submit form
        const response = await fetch('/api/process', {
            method: 'POST',
            body: formData
        });
        
        const data = await response.json();
        
        if (data.video_id) {
            currentVideoId = data.video_id;
            startProgressTracking();
        } else {
            throw new Error(data.error || 'Failed to start processing');
        }
        
    } catch (error) {
        showError('Failed to start processing: ' + error.message);
        resetUI();
    }
}

/**
 * Validate form before submission
 */
function validateForm() {
    const hasVideo = elements.videoUpload.files.length > 0;
    const hasUrl = elements.ytUrl.value.trim() !== '';
    
    if (!hasVideo && !hasUrl) {
        showError('Please upload a video file or provide a YouTube URL');
        return false;
    }
    
    if (hasVideo && hasUrl) {
        showError('Please provide either a video file OR a YouTube URL, not both');
        return false;
    }
    
    // Validate file size if uploading
    if (hasVideo) {
        const file = elements.videoUpload.files[0];
        const maxSize = 100 * 1024 * 1024; // 100MB
        
        if (file.size > maxSize) {
            showError('File size must be less than 100MB');
            return false;
        }
    }
    
    // Validate YouTube URL format
    if (hasUrl) {
        const ytUrlPattern = /^(https?:\/\/)?(www\.)?(youtube\.com\/watch\?v=|youtu\.be\/)[\w-]+/;
        if (!ytUrlPattern.test(elements.ytUrl.value.trim())) {
            showError('Please provide a valid YouTube URL');
            return false;
        }
    }
    
    return true;
}

/**
 * Handle file selection for upload
 */
function handleFileSelection(e) {
    const file = e.target.files[0];
    if (file) {
        // Clear YouTube URL if file is selected
        elements.ytUrl.value = '';
        
        // Update UI to show selected file
        const label = document.querySelector('.upload-label span');
        label.textContent = `Selected: ${file.name}`;
        label.parentElement.classList.add('file-selected');
    }
}

/**
 * Update voice options based on selected target language
 */
function updateVoiceOptions() {
    const targetLang = elements.targetLanguage.value;
    const voiceSelect = elements.voice;
    
    // Clear existing options except auto
    while (voiceSelect.children.length > 1) {
        voiceSelect.removeChild(voiceSelect.lastChild);
    }
    
    // Voice mapping for different languages
    const voiceOptions = {
        'english': [
            { value: 'en-US-Standard-A', text: 'üá∫üá∏ US English (Male)' },
            { value: 'en-US-Standard-C', text: 'üá∫üá∏ US English (Female)' },
            { value: 'en-GB-Standard-A', text: 'üá¨üáß British English (Female)' },
            { value: 'en-AU-Standard-A', text: 'üá¶üá∫ Australian English (Female)' }
        ],
        'hindi': [
            { value: 'hi-IN-Standard-A', text: 'üáÆüá≥ Hindi (Female)' },
            { value: 'hi-IN-Standard-B', text: 'üáÆüá≥ Hindi (Male)' }
        ],
        'bengali': [
            { value: 'bn-IN-Standard-A', text: 'üáÆüá≥ Bengali (Female)' },
            { value: 'bn-IN-Standard-B', text: 'üáÆüá≥ Bengali (Male)' }
        ],
        'tamil': [
            { value: 'ta-IN-Standard-A', text: 'üáÆüá≥ Tamil (Female)' },
            { value: 'ta-IN-Standard-B', text: 'üáÆüá≥ Tamil (Male)' }
        ],
        // Add more languages as needed
    };
    
    // Add language-specific voices
    const voices = voiceOptions[targetLang] || [
        { value: 'auto-' + targetLang, text: `üéØ Best ${targetLang.charAt(0).toUpperCase() + targetLang.slice(1)} Voice` }
    ];
    
    voices.forEach(voice => {
        const option = document.createElement('option');
        option.value = voice.value;
        option.textContent = voice.text;
        voiceSelect.appendChild(option);
    });
}

/**
 * Show processing UI and hide form
 */
function showProcessingUI() {
    elements.form.parentElement.style.display = 'none';
    elements.loader.classList.remove('hidden');
    elements.resultContainer.classList.add('hidden');
    
    // Reset processing steps
    resetProcessingSteps();
    updateProgress(0, 'Initializing...');
}

/**
 * Reset all processing step indicators
 */
function resetProcessingSteps() {
    Object.values(PROCESSING_STEPS).forEach(step => {
        if (step.element) {
            step.element.classList.remove('active', 'completed');
                }
            });
    }

/**
 * Start tracking processing progress
 */
function startProgressTracking() {
    if (processingInterval) {
        clearInterval(processingInterval);
    }
    
    processingInterval = setInterval(async () => {
        try {
            const response = await fetch(`/api/status/${currentVideoId}`);
            const status = await response.json();
            
            updateProcessingStatus(status);
            
            if (status.status === 'completed') {
                clearInterval(processingInterval);
                processingInterval = null;
                showResults(status);
            } else if (status.status === 'error') {
                clearInterval(processingInterval);
                processingInterval = null;
                showError(status.error || 'Processing failed');
                resetUI();
            }
            
        } catch (error) {
            console.error('Error checking status:', error);
        }
    }, 2000); // Check every 2 seconds
}

/**
 * Update processing status display
 */
function updateProcessingStatus(status) {
    const progress = Math.min(status.progress || 0, 100);
    const message = status.message || 'Processing...';
    const currentStatus = status.status || 'processing';
    
    updateProgress(progress, message);
    updateProcessingSteps(currentStatus, progress);
}

/**
 * Update progress bar and percentage
 */
function updateProgress(percentage, message) {
    elements.progressPercentage.textContent = `${Math.round(percentage)}%`;
    elements.progressFill.style.width = `${percentage}%`;
    elements.processingMessage.textContent = message;
}

/**
 * Update processing step indicators
 */
function updateProcessingSteps(status, progress) {
    const stepConfig = PROCESSING_STEPS[status];
    
    if (stepConfig && stepConfig.element) {
        // Mark current step as active
        stepConfig.element.classList.add('active');
        
        // Mark previous steps as completed
        Object.values(PROCESSING_STEPS).forEach(step => {
            if (step.element && step.progress[1] <= progress) {
                step.element.classList.add('completed');
                step.element.classList.remove('active');
                }
            });
        }
}

/**
 * Show results after processing completion
 */
function showResults(data) {
    elements.loader.classList.add('hidden');
    elements.resultContainer.classList.remove('hidden');
    
    summaryData = data;
    
    // Populate topics grid
    populateTopicsGrid(data.summaries, data.keywords);
    
    // Show success message
    showSuccess('Video processing completed successfully!');
}

/**
 * Populate the topics grid with available topics
 */
function populateTopicsGrid(summaries, keywords) {
    elements.topicsGrid.innerHTML = '';
    
    summaries.forEach((summary, index) => {
        const topicCard = createTopicCard(index, summary, keywords[index] || []);
        elements.topicsGrid.appendChild(topicCard);
    });
}

/**
 * Create a topic card element
 */
function createTopicCard(index, summary, topicKeywords) {
    const card = document.createElement('div');
    card.className = 'topic-card';
    card.setAttribute('data-topic-index', index);
    
    // Generate topic title from keywords or use default
    const topicTitle = topicKeywords.length > 0 
        ? topicKeywords.slice(0, 3).join(', ')
        : `Topic ${index + 1}`;
    
    card.innerHTML = `
        <div class="topic-header">
            <h4><i class="fas fa-play-circle"></i> ${topicTitle}</h4>
            <span class="topic-duration">${estimateDuration(summary)}</span>
        </div>
        <div class="topic-preview">
            <p>${truncateText(summary, 100)}</p>
        </div>
        <div class="topic-keywords">
            ${topicKeywords.slice(0, 5).map(keyword => 
                `<span class="keyword-tag">${keyword}</span>`
            ).join('')}
        </div>
        <button class="play-topic-btn" onclick="playTopic(${index})">
            <i class="fas fa-play"></i> Play Summary
        </button>
    `;
    
    return card;
}

/**
 * Play a specific topic summary
 */
function playTopic(topicIndex) {
    if (!summaryData || !summaryData.summary_videos[topicIndex]) {
        showError('Topic video not available');
        return;
    }
    
    const summary = summaryData.summaries[topicIndex];
    const keywords = summaryData.keywords[topicIndex] || [];
    const videoPath = summaryData.summary_videos[topicIndex];
    
    // Update topic player
    elements.currentTopicTitle.textContent = keywords.length > 0 
        ? keywords.slice(0, 3).join(', ')
        : `Topic ${topicIndex + 1}`;
    
    elements.topicSummaryText.textContent = summary;
    
    // Populate keywords
    elements.topicKeywordsList.innerHTML = keywords
        .map(keyword => `<span class="keyword-tag">${keyword}</span>`)
        .join('');
    
    // Set video source
    elements.topicVideo.src = `/api/stream/${currentVideoId}/${topicIndex}`;
    
    // Show player
    elements.topicPlayer.classList.remove('hidden');
    
    // Scroll to player
    elements.topicPlayer.scrollIntoView({ behavior: 'smooth' });
}

/**
 * Close the topic player
 */
function closeTopicPlayer() {
    elements.topicPlayer.classList.add('hidden');
    elements.topicVideo.pause();
    elements.topicVideo.src = '';
}

/**
 * Download all summaries as a ZIP file
 */
function downloadAllSummaries() {
    if (!currentVideoId) return;
    
    // Create download link
    const link = document.createElement('a');
    link.href = `/api/download/all/${currentVideoId}`;
    link.download = `video_summaries_${currentVideoId}.zip`;
    link.click();
}

/**
 * Download transcript file
 */
function downloadTranscript() {
    if (!currentVideoId) return;
    
    const link = document.createElement('a');
    link.href = `/srt/${currentVideoId}`;
    link.download = `transcript_${currentVideoId}.srt`;
    link.click();
}

/**
 * Download keyframes
 */
function downloadKeyframes() {
    if (!currentVideoId) return;
    
    const link = document.createElement('a');
    link.href = `/api/download/keyframes/${currentVideoId}`;
    link.download = `keyframes_${currentVideoId}.zip`;
    link.click();
}

/**
 * Handle keyboard shortcuts
 */
function handleKeyboardShortcuts(e) {
    // Escape key to close player
    if (e.key === 'Escape' && !elements.topicPlayer.classList.contains('hidden')) {
        closeTopicPlayer();
    }
    
    // Space key to play/pause current video
    if (e.key === ' ' && !elements.topicPlayer.classList.contains('hidden')) {
        e.preventDefault();
        if (elements.topicVideo.paused) {
            elements.topicVideo.play();
        } else {
            elements.topicVideo.pause();
        }
    }
}

/**
 * Utility Functions
 */

function truncateText(text, maxLength) {
    return text.length > maxLength ? text.substring(0, maxLength) + '...' : text;
}

function estimateDuration(text) {
    // Rough estimate: 150 words per minute for speech
    const words = text.split(' ').length;
    const minutes = Math.ceil(words / 150);
    return `~${minutes} min`;
}

function showError(message) {
    showNotification(message, 'error');
}

function showSuccess(message) {
    showNotification(message, 'success');
}

function showNotification(message, type = 'info') {
    // Create notification element
    const notification = document.createElement('div');
    notification.className = `notification notification-${type}`;
    notification.innerHTML = `
        <i class="fas fa-${type === 'error' ? 'exclamation-circle' : type === 'success' ? 'check-circle' : 'info-circle'}"></i>
        <span>${message}</span>
        <button class="close-notification" onclick="this.parentElement.remove()">
            <i class="fas fa-times"></i>
        </button>
    `;
    
    // Add to page
    document.body.appendChild(notification);
    
    // Auto-remove after 5 seconds
    setTimeout(() => {
        if (notification.parentElement) {
            notification.remove();
        }
    }, 5000);
}

function showTooltip(element, message) {
    // Simple tooltip implementation
    const tooltip = document.createElement('div');
    tooltip.className = 'tooltip';
    tooltip.textContent = message;
    document.body.appendChild(tooltip);
    
    const rect = element.getBoundingClientRect();
    tooltip.style.left = rect.left + 'px';
    tooltip.style.top = (rect.bottom + 5) + 'px';
    
    setTimeout(() => tooltip.remove(), 3000);
}

function resetUI() {
    elements.form.parentElement.style.display = 'block';
    elements.loader.classList.add('hidden');
    elements.resultContainer.classList.add('hidden');
    
    if (processingInterval) {
        clearInterval(processingInterval);
        processingInterval = null;
    }
    
    currentVideoId = null;
    summaryData = null;
}
</file>

<file path="static/style.css">
/* General Body Styles */
body {
    font-family: 'Poppins', sans-serif;
    background-color: #f0f2f5;
    color: #333;
    margin: 0;
    padding: 20px;
    display: flex;
    justify-content: center;
    align-items: center;
    min-height: 100vh;
}

.container {
    width: 100%;
    max-width: 800px;
    text-align: center;
}

header h1 {
    font-size: 2.5rem;
    color: #1a237e;
    margin-bottom: 10px;
}

header p {
    font-size: 1.1rem;
    color: #555;
    margin-bottom: 30px;
}

/* Card Styles */
.card {
    background: #ffffff;
    padding: 40px;
    border-radius: 12px;
    box-shadow: 0 8px 30px rgba(0, 0, 0, 0.1);
    margin-bottom: 30px;
}

.form-group {
    margin-bottom: 20px;
}

.form-group label {
    display: block;
    font-weight: 600;
    margin-bottom: 8px;
    color: #333;
}

.form-group input[type="text"],
.form-group input[type="file"] {
    width: 100%;
    padding: 12px;
    border: 1px solid #ddd;
    border-radius: 8px;
    box-sizing: border-box;
}

.form-group input[type="file"] {
    padding: 10px;
}

.separator {
    margin: 25px 0;
    font-weight: 600;
    color: #999;
}

.submit-btn {
    width: 100%;
    padding: 15px;
    background: linear-gradient(45deg, #3d5afe, #1e88e5);
    color: white;
    border: none;
    border-radius: 8px;
    font-size: 1.1rem;
    font-weight: 600;
    cursor: pointer;
    transition: all 0.3s ease;
}

.submit-btn:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 15px rgba(61, 90, 254, 0.4);
}

/* Loader/Spinner Styles */
#loader {
    text-align: center;
}

.spinner {
    border: 6px solid #f3f3f3;
    border-top: 6px solid #3d5afe;
    border-radius: 50%;
    width: 50px;
    height: 50px;
    animation: spin 1s linear infinite;
    margin: 0 auto 15px auto;
}

@keyframes spin {
    0% { transform: rotate(0deg); }
    100% { transform: rotate(360deg); }
}

#loader p {
    font-size: 1.1rem;
    color: #555;
}

/* Results Section */
#result-container {
    background: #ffffff;
    padding: 30px;
    border-radius: 12px;
    box-shadow: 0 8px 30px rgba(0, 0, 0, 0.1);
    text-align: left;
}

#result-container h2 {
    color: #1a237e;
    border-bottom: 2px solid #eee;
    padding-bottom: 10px;
    margin-bottom: 20px;
}

.result-section {
    margin-bottom: 30px;
}

.topic-btn {
    background-color: #e8eaf6;
    color: #3d5afe;
    border: 1px solid #c5cae9;
    padding: 8px 15px;
    margin: 5px;
    border-radius: 20px;
    cursor: pointer;
    transition: all 0.2s ease;
}

.topic-btn:hover {
    background-color: #c5cae9;
}

video, audio {
    width: 100%;
    border-radius: 8px;
    margin-top: 10px;
}

.download-links a {
    color: #3d5afe;
    text-decoration: none;
    margin-right: 15px;
}

.download-links a:hover {
    text-decoration: underline;
}

.hidden {
    display: none;
}
</file>

<file path="templates/index.html">
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>üé¨ AI Video Summarizer - Multi-Language Support</title>
    <link rel="stylesheet" href="{{ url_for('static', filename='style.css') }}">
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
</head>
<body>
    <div class="container">
        <header>
            <div class="header-content">
                <h1><i class="fas fa-video"></i> AI Video Summarizer</h1>
                <p class="subtitle">Transform long videos into concise, intelligent summaries with multi-language support</p>
                <div class="features-pills">
                    <span class="pill"><i class="fas fa-language"></i> 22+ Indian Languages</span>
                    <span class="pill"><i class="fas fa-brain"></i> AI-Powered</span>
                    <span class="pill"><i class="fas fa-magic"></i> Topic-Based</span>
                    <span class="pill"><i class="fas fa-microphone"></i> Voice Generation</span>
                </div>
            </div>
        </header>
        
        <div class="main-card">
            <form id="main-form" enctype="multipart/form-data">
                <!-- Video Input Section -->
                <div class="input-section">
                    <h3><i class="fas fa-upload"></i> Video Input</h3>
                    <div class="input-methods">
                        <div class="input-method">
                            <label for="video-upload" class="upload-label">
                                <i class="fas fa-cloud-upload-alt"></i>
                                <span>Upload Video File</span>
                                <small>Max 100MB, up to 60 minutes</small>
                            </label>
                            <input type="file" id="video-upload" name="video" accept="video/*" style="display: none;">
                        </div>
                        
                        <div class="separator">
                            <span>OR</span>
                        </div>
                        
                        <div class="input-method">
                            <label for="yt-url"><i class="fab fa-youtube"></i> YouTube URL</label>
                            <input type="url" id="yt-url" name="yt_url" placeholder="https://www.youtube.com/watch?v=..." class="form-control">
                        </div>
                    </div>
                </div>

                <!-- Language Configuration Section -->
                <div class="config-section">
                    <h3><i class="fas fa-language"></i> Language Configuration</h3>
                    <div class="language-grid">
                        <div class="form-group">
                            <label for="source-language"><i class="fas fa-microphone"></i> Source Language</label>
                            <select id="source-language" name="source_language" class="form-control">
                                <option value="auto">üîç Auto-detect</option>
                                <optgroup label="üáÆüá≥ Indian Languages">
                                    <option value="hindi">üáÆüá≥ ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (Hindi)</option>
                                    <option value="bengali">üáÆüá≥ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ (Bengali)</option>
                                    <option value="telugu">üáÆüá≥ ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å (Telugu)</option>
                                    <option value="marathi">üáÆüá≥ ‡§Æ‡§∞‡§æ‡§†‡•Ä (Marathi)</option>
                                    <option value="tamil">üáÆüá≥ ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç (Tamil)</option>
                                    <option value="gujarati">üáÆüá≥ ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä (Gujarati)</option>
                                    <option value="urdu">üáÆüá≥ ÿßÿ±ÿØŸà (Urdu)</option>
                                    <option value="kannada">üáÆüá≥ ‡≤ï‡≤®‡≥ç‡≤®‡≤° (Kannada)</option>
                                    <option value="malayalam">üáÆüá≥ ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç (Malayalam)</option>
                                    <option value="punjabi">üáÆüá≥ ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä (Punjabi)</option>
                                    <option value="odia">üáÆüá≥ ‡¨ì‡¨°‡¨º‡¨ø‡¨Ü (Odia)</option>
                                    <option value="assamese">üáÆüá≥ ‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ (Assamese)</option>
                                    <option value="nepali">üá≥üáµ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä (Nepali)</option>
                                    <option value="sanskrit">üáÆüá≥ ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§ (Sanskrit)</option>
                                </optgroup>
                                <optgroup label="üåç International Languages">
                                    <option value="english">üá∫üá∏ English</option>
                                    <option value="arabic">üá∏üá¶ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)</option>
                                    <option value="chinese">üá®üá≥ ‰∏≠Êñá (Chinese)</option>
                                    <option value="spanish">üá™üá∏ Espa√±ol (Spanish)</option>
                                    <option value="french">üá´üá∑ Fran√ßais (French)</option>
                                    <option value="german">üá©üá™ Deutsch (German)</option>
                                    <option value="japanese">üáØüáµ Êó•Êú¨Ë™û (Japanese)</option>
                                    <option value="korean">üá∞üá∑ ÌïúÍµ≠Ïñ¥ (Korean)</option>
                                    <option value="russian">üá∑üá∫ –†—É—Å—Å–∫–∏–π (Russian)</option>
                                    <option value="portuguese">üáßüá∑ Portugu√™s (Portuguese)</option>
                                </optgroup>
                            </select>
                        </div>
                        
                        <div class="form-group">
                            <label for="target-language"><i class="fas fa-bullhorn"></i> Summary Language</label>
                            <select id="target-language" name="target_language" class="form-control">
                                <optgroup label="üáÆüá≥ Indian Languages">
                                    <option value="hindi">üáÆüá≥ ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä (Hindi)</option>
                                    <option value="bengali">üáÆüá≥ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ (Bengali)</option>
                                    <option value="telugu">üáÆüá≥ ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å (Telugu)</option>
                                    <option value="marathi">üáÆüá≥ ‡§Æ‡§∞‡§æ‡§†‡•Ä (Marathi)</option>
                                    <option value="tamil">üáÆüá≥ ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç (Tamil)</option>
                                    <option value="gujarati">üáÆüá≥ ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä (Gujarati)</option>
                                    <option value="urdu">üáÆüá≥ ÿßÿ±ÿØŸà (Urdu)</option>
                                    <option value="kannada">üáÆüá≥ ‡≤ï‡≤®‡≥ç‡≤®‡≤° (Kannada)</option>
                                    <option value="malayalam">üáÆüá≥ ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç (Malayalam)</option>
                                    <option value="punjabi">üáÆüá≥ ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä (Punjabi)</option>
                                    <option value="odia">üáÆüá≥ ‡¨ì‡¨°‡¨º‡¨ø‡¨Ü (Odia)</option>
                                    <option value="assamese">üáÆüá≥ ‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ (Assamese)</option>
                                    <option value="nepali">üá≥üáµ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä (Nepali)</option>
                                    <option value="sanskrit">üáÆüá≥ ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§ (Sanskrit)</option>
                                </optgroup>
                                <optgroup label="üåç International Languages">
                                    <option value="english" selected>üá∫üá∏ English</option>
                                    <option value="arabic">üá∏üá¶ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© (Arabic)</option>
                                    <option value="chinese">üá®üá≥ ‰∏≠Êñá (Chinese)</option>
                                    <option value="spanish">üá™üá∏ Espa√±ol (Spanish)</option>
                                    <option value="french">üá´üá∑ Fran√ßais (French)</option>
                                    <option value="german">üá©üá™ Deutsch (German)</option>
                                    <option value="japanese">üáØüáµ Êó•Êú¨Ë™û (Japanese)</option>
                                    <option value="korean">üá∞üá∑ ÌïúÍµ≠Ïñ¥ (Korean)</option>
                                    <option value="russian">üá∑üá∫ –†—É—Å—Å–∫–∏–π (Russian)</option>
                                    <option value="portuguese">üáßüá∑ Portugu√™s (Portuguese)</option>
                                </optgroup>
                            </select>
                        </div>
                    </div>
                </div>

                <!-- Output Configuration Section -->
                <div class="config-section">
                    <h3><i class="fas fa-cogs"></i> Output Configuration</h3>
                    <div class="config-grid">
                        <div class="form-group">
                            <label for="voice"><i class="fas fa-volume-up"></i> Voice Style</label>
                            <select id="voice" name="voice" class="form-control">
                                <option value="auto">üéØ Auto-select Best Voice</option>
                                <optgroup label="English Voices">
                                    <option value="en-US-Standard-A">üá∫üá∏ US English (Male)</option>
                                    <option value="en-US-Standard-C" selected>üá∫üá∏ US English (Female)</option>
                                    <option value="en-GB-Standard-A">üá¨üáß British English (Female)</option>
                                    <option value="en-AU-Standard-A">üá¶üá∫ Australian English (Female)</option>
                                </optgroup>
                                <optgroup label="Hindi Voices">
                                    <option value="hi-IN-Standard-A">üáÆüá≥ Hindi (Female)</option>
                                    <option value="hi-IN-Standard-B">üáÆüá≥ Hindi (Male)</option>
                                </optgroup>
                                <!-- More voices will be populated dynamically -->
                            </select>
                        </div>
                        
                        <div class="form-group">
                            <label for="resolution"><i class="fas fa-tv"></i> Video Quality</label>
                            <select id="resolution" name="resolution" class="form-control">
                                <option value="480p" selected>üì± 480p (Fast, Mobile-friendly)</option>
                                <option value="720p">üíª 720p (Balanced Quality)</option>
                                <option value="1080p">üñ•Ô∏è 1080p (High Quality)</option>
                            </select>
                        </div>
                        
                        <div class="form-group">
                            <label for="summary_length"><i class="fas fa-clock"></i> Summary Length</label>
                            <select id="summary_length" name="summary_length" class="form-control">
                                <option value="short">‚ö° Short (1-2 minutes)</option>
                                <option value="medium" selected>‚öñÔ∏è Medium (3-5 minutes)</option>
                                <option value="long">üìö Long (5-10 minutes)</option>
                            </select>
                        </div>
                        
                        <div class="form-group">
                            <label for="enable_ocr">
                                <input type="checkbox" id="enable_ocr" name="enable_ocr" checked>
                                <i class="fas fa-text-width"></i> Extract Text from Video
                            </label>
                            <small>Analyze text in video frames for better context</small>
                        </div>
                    </div>
                </div>

                <!-- Submit Section -->
                <div class="submit-section">
                    <button type="submit" class="submit-btn">
                        <i class="fas fa-play-circle"></i>
                        <span>Process Video</span>
                    </button>
                    <p class="processing-note">
                        <i class="fas fa-info-circle"></i>
                        Processing typically takes 20-30% of the original video length
                    </p>
                </div>
            </form>
        </div>

        <!-- Processing Status -->
        <div id="loader" class="hidden">
            <div class="processing-card">
                <div class="spinner-container">
                    <div class="spinner"></div>
                    <div class="progress-circle">
                        <span id="progress-percentage">0%</span>
                    </div>
                </div>
                <h3 id="processing-title">Processing your video...</h3>
                <p id="processing-message">Initializing...</p>
                <div class="progress-bar">
                    <div id="progress-fill"></div>
                </div>
                <div class="processing-steps">
                    <div class="step" id="step-download">
                        <i class="fas fa-download"></i>
                        <span>Download/Upload</span>
                    </div>
                    <div class="step" id="step-transcribe">
                        <i class="fas fa-microphone"></i>
                        <span>Transcription</span>
                    </div>
                    <div class="step" id="step-keyframes">
                        <i class="fas fa-images"></i>
                        <span>Keyframes</span>
                    </div>
                    <div class="step" id="step-analyze">
                        <i class="fas fa-brain"></i>
                        <span>Analysis</span>
                    </div>
                    <div class="step" id="step-generate">
                        <i class="fas fa-video"></i>
                        <span>Generation</span>
                    </div>
                </div>
            </div>
        </div>

        <!-- Results Section -->
        <div id="result-container" class="hidden">
            <div class="results-header">
                <h2><i class="fas fa-check-circle"></i> Processing Complete!</h2>
                <p>Your video has been successfully summarized. Select a topic below to view the summary:</p>
            </div>
            
            <div id="topics-grid" class="topics-grid">
                <!-- Topics will be populated dynamically -->
            </div>
            
            <div id="topic-player" class="topic-player hidden">
                <div class="player-header">
                    <h3 id="current-topic-title">Topic Title</h3>
                    <button id="close-player" class="close-btn">
                        <i class="fas fa-times"></i>
                    </button>
                </div>
                <div class="player-content">
                    <video id="topic-video" controls preload="metadata">
                        Your browser does not support the video tag.
                    </video>
                    <div class="topic-info">
                        <div class="topic-summary">
                            <h4><i class="fas fa-file-text"></i> Summary</h4>
                            <p id="topic-summary-text">Summary will appear here...</p>
                        </div>
                        <div class="topic-keywords">
                            <h4><i class="fas fa-tags"></i> Key Topics</h4>
                            <div id="topic-keywords-list">
                                <!-- Keywords will be populated dynamically -->
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            
            <div class="download-section">
                <h3><i class="fas fa-download"></i> Download Options</h3>
                <div class="download-buttons">
                    <button id="download-all" class="download-btn">
                        <i class="fas fa-file-archive"></i>
                        Download All Summaries
                    </button>
                    <button id="download-transcript" class="download-btn">
                        <i class="fas fa-file-text"></i>
                        Download Transcript
                    </button>
                    <button id="download-keyframes" class="download-btn">
                        <i class="fas fa-images"></i>
                        Download Keyframes
                    </button>
                </div>
            </div>
        </div>
    </div>

    <script src="{{ url_for('static', filename='script.js') }}"></script>
</body>
</html>
</file>

<file path="uploads/.gitkeep">
# This file ensures the uploads directory is tracked by git
</file>

<file path="utils/clustering.py">
import logging
from typing import List, Dict, Any, Tuple, Optional
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation, NMF
from sklearn.cluster import KMeans, DBSCAN
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import string
import re
from collections import defaultdict

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
    nltk.data.find('corpora/stopwords')
    nltk.data.find('corpora/wordnet')
except LookupError:
    logger.info("Downloading NLTK data...")
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('wordnet')

# Language-specific settings
LANGUAGE_STOPWORDS = {
    'en': set(stopwords.words('english')),
    'hi': set(stopwords.words('hindi') if 'hindi' in stopwords.fileids() else []),
    'kn': set(stopwords.words('kannada') if 'kannada' in stopwords.fileids() else [])
}

# Add custom stopwords for each language
CUSTOM_STOPWORDS = {
    'en': {'like', 'one', 'would', 'get', 'also', 'could', 'may', 'even', 'much', 'many', 'well'},
    'hi': set(),
    'kn': set()
}

# Combine NLTK and custom stopwords
for lang in LANGUAGE_STOPWORDS:
    LANGUAGE_STOPWORDS[lang].update(CUSTOM_STOPWORDS.get(lang, set()))

class TextPreprocessor:
    """Text preprocessing utilities for clustering."""
    
    def __init__(self, language: str = 'en'):
        """Initialize the text preprocessor.
        
        Args:
            language: Language code ('en', 'hi', 'kn')
        """
        self.language = language
        self.lemmatizer = WordNetLemmatizer()
        self.stopwords = LANGUAGE_STOPWORDS.get(language, set())
        
    def clean_text(self, text: str) -> str:
        """Clean and preprocess text."""
        if not isinstance(text, str):
            return ""
            
        # Convert to lowercase
        text = text.lower()
        
        # Remove URLs
        text = re.sub(r'https?://\S+|www\.\S+', '', text)
        
        # Remove HTML tags
        text = re.sub(r'<.*?>', '', text)
        
        # Remove punctuation (language-specific handling)
        if self.language == 'en':
            # Keep sentence boundaries for English
            text = re.sub(r'[^\w\s.!?]', '', text)
        else:
            # For other languages, be more conservative with punctuation removal
            text = re.sub(r'[^\w\s]', ' ', text)
        
        # Remove extra whitespace
        text = ' '.join(text.split())
        
        return text
    
    def tokenize(self, text: str) -> List[str]:
        """Tokenize text into words."""
        if not text.strip():
            return []
            
        # Tokenize
        tokens = word_tokenize(text, language='english' if self.language == 'en' else self.language)
        
        # Remove stopwords and short tokens
        tokens = [
            token for token in tokens 
            if token not in self.stopwords 
            and len(token) > 2
            and not token.isdigit()
        ]
        
        # Lemmatization (for English only)
        if self.language == 'en':
            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]
        
        return tokens
    
    def preprocess(self, text: str) -> str:
        """Full preprocessing pipeline."""
        cleaned = self.clean_text(text)
        tokens = self.tokenize(cleaned)
        return ' '.join(tokens)

def cluster_segments(
    segments: List[Dict[str, Any]],
    n_clusters: int = 5,
    method: str = 'lda',
    language: str = 'en',
    min_topic_size: int = 2,
    max_features: int = 5000
) -> List[List[Dict[str, Any]]]:
    """Cluster text segments into topics.
    
    Args:
        segments: List of text segments with 'text' keys
        n_clusters: Number of clusters/topics to create
        method: Clustering method ('lda', 'nmf', 'kmeans', 'dbscan')
        language: Language code ('en', 'hi', 'kn')
        min_topic_size: Minimum number of segments per topic
        max_features: Maximum number of features for vectorization
        
    Returns:
        List of clusters, where each cluster is a list of segments
    """
    if not segments:
        return []
    
    # Validate language
    language = language.lower()
    if language not in ['en', 'hi', 'kn']:
        logger.warning(f"Unsupported language: {language}. Defaulting to English.")
        language = 'en'
    
    try:
        # Extract texts from segments
        texts = [seg.get('text', '') for seg in segments]
        
        # Initialize preprocessor
        preprocessor = TextPreprocessor(language)
        
        # Preprocess texts
        preprocessed_texts = [preprocessor.preprocess(text) for text in texts]
        
        # Vectorize texts
        if method in ['lda', 'nmf']:
            # For topic modeling, use count vectorizer
            vectorizer = CountVectorizer(
                max_features=max_features,
                stop_words=list(LANGUAGE_STOPWORDS[language]) if language in LANGUAGE_STOPWORDS else None
            )
            X = vectorizer.fit_transform(preprocessed_texts)
            
            # Apply topic modeling
            if method == 'lda':
                model = LatentDirichletAllocation(
                    n_components=n_clusters,
                    random_state=42,
                    learning_method='online',
                    max_iter=10
                )
                topic_assignments = model.fit_transform(X).argmax(axis=1)
            else:  # NMF
                model = NMF(
                    n_components=n_clusters,
                    random_state=42,
                    max_iter=1000
                )
                topic_assignments = model.fit_transform(X).argmax(axis=1)
                
        else:  # kmeans or dbscan
            # For clustering, use TF-IDF
            vectorizer = TfidfVectorizer(
                max_features=max_features,
                stop_words=list(LANGUAGE_STOPWORDS[language]) if language in LANGUAGE_STOPWORDS else None
            )
            X = vectorizer.fit_transform(preprocessed_texts)
            
            if method == 'kmeans':
                model = KMeans(
                    n_clusters=n_clusters,
                    random_state=42,
                    n_init=10
                )
                topic_assignments = model.fit_predict(X)
            else:  # dbscan
                model = DBSCAN(
                    eps=0.5,
                    min_samples=2,
                    metric='cosine'
                )
                topic_assignments = model.fit_predict(X)
                
                # Handle noise points (assigned to -1)
                if -1 in topic_assignments:
                    # Assign noise points to their own clusters
                    max_cluster = max(topic_assignments) + 1
                    topic_assignments = [x if x != -1 else max_cluster + i for i, x in enumerate(topic_assignments)]
        
        # Assign cluster labels to segments
        for i, seg in enumerate(segments):
            seg['cluster'] = int(topic_assignments[i])
        
        # Group segments by cluster
        clusters = defaultdict(list)
        for seg in segments:
            clusters[seg['cluster']].append(seg)
        
        # Filter small clusters
        filtered_clusters = [
            cluster for cluster in clusters.values() 
            if len(cluster) >= min_topic_size
        ]
        
        # Sort clusters by size (largest first)
        filtered_clusters.sort(key=len, reverse=True)
        
        # Limit to n_clusters if we have more
        if len(filtered_clusters) > n_clusters:
            filtered_clusters = filtered_clusters[:n_clusters]
        
        logger.info(f"Created {len(filtered_clusters)} clusters with {sum(len(c) for c in filtered_clusters)} segments")
        
        return filtered_clusters
        
    except Exception as e:
        logger.error(f"Error in cluster_segments: {e}", exc_info=True)
        
        # Fallback: return all segments in a single cluster
        logger.warning("Falling back to single cluster")
        return [segments]

def extract_keywords(
    texts: List[str],
    n_keywords: int = 5,
    language: str = 'en',
    max_features: int = 5000
) -> List[str]:
    """Extract keywords from a list of texts."""
    if not texts:
        return []
    
    try:
        # Preprocess texts
        preprocessor = TextPreprocessor(language)
        preprocessed_texts = [' '.join(preprocessor.tokenize(text)) for text in texts]
        
        # Vectorize using TF-IDF
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            stop_words=list(LANGUAGE_STOPWORDS[language]) if language in LANGUAGE_STOPWORDS else None
        )
        X = vectorizer.fit_transform(preprocessed_texts)
        
        # Get feature names (words)
        feature_names = vectorizer.get_feature_names_out()
        
        # Calculate average TF-IDF score for each word across all documents
        avg_tfidf = X.mean(axis=0).A1
        
        # Sort words by average TF-IDF score
        top_indices = avg_tfidf.argsort()[-n_keywords:][::-1]
        
        # Extract top keywords
        keywords = [feature_names[i] for i in top_indices if i < len(feature_names)]
        
        return keywords
    
    except Exception as e:
        logger.error(f"Error in extract_keywords: {e}", exc_info=True)
        return []
</file>

<file path="utils/keyframes.py">
"""
Enhanced Keyframe Extraction and OCR Analysis Module

This module provides comprehensive keyframe extraction with OCR capabilities
to extract text content from video frames for better context understanding.
It supports multiple OCR engines and text analysis features.

Author: Video Summarizer Team
Created: 2024
"""

import os
import cv2
import numpy as np
from typing import List, Tuple, Optional, Dict, Any
import logging
import json
from pathlib import Path
import time

# OCR Dependencies
try:
    import pytesseract
    TESSERACT_AVAILABLE = True
except ImportError:
    TESSERACT_AVAILABLE = False
    
try:
    import easyocr
    EASYOCR_AVAILABLE = True
except ImportError:
    EASYOCR_AVAILABLE = False

try:
    from PIL import Image
    PIL_AVAILABLE = True
except ImportError:
    PIL_AVAILABLE = False

# Text processing
import re
from collections import Counter

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# OCR Configuration for different languages
OCR_LANGUAGES = {
    'english': 'eng',
    'hindi': 'hin',
    'bengali': 'ben',
    'telugu': 'tel',
    'marathi': 'mar',
    'tamil': 'tam',
    'gujarati': 'guj',
    'urdu': 'urd',
    'kannada': 'kan',
    'odia': 'ori',
    'malayalam': 'mal',
    'punjabi': 'pan',
    'assamese': 'asm',
    'nepali': 'nep',
    'sanskrit': 'san',
}

# EasyOCR language codes
EASYOCR_LANGUAGES = {
    'english': 'en',
    'hindi': 'hi',
    'bengali': 'bn',
    'tamil': 'ta',
    'korean': 'ko',
    'chinese': 'ch_sim',
    'japanese': 'ja',
    'arabic': 'ar',
}

class KeyframeOCR:
    """
    OCR engine for extracting text from keyframes.
    Supports multiple OCR backends with language-specific optimization.
    """
    
    def __init__(self, languages: List[str] = ['english'], ocr_engine: str = 'auto'):
        """
        Initialize OCR engine.
        
        Args:
            languages: List of languages to detect
            ocr_engine: OCR engine to use ('tesseract', 'easyocr', 'auto')
        """
        self.languages = languages
        self.ocr_engine = ocr_engine
        self.easyocr_reader = None
        
        # Initialize EasyOCR if available and requested
        if EASYOCR_AVAILABLE and ocr_engine in ['easyocr', 'auto']:
            try:
                # Convert language names to EasyOCR codes
                easyocr_langs = []
                for lang in languages:
                    if lang in EASYOCR_LANGUAGES:
                        easyocr_langs.append(EASYOCR_LANGUAGES[lang])
                
                if easyocr_langs:
                    self.easyocr_reader = easyocr.Reader(easyocr_langs)
                    logger.info(f"EasyOCR initialized with languages: {easyocr_langs}")
            except Exception as e:
                logger.warning(f"Failed to initialize EasyOCR: {e}")
    
    def preprocess_image(self, image: np.ndarray) -> np.ndarray:
        """
        Preprocess image for better OCR accuracy.
        
        Args:
            image: Input image as numpy array
            
        Returns:
            Preprocessed image
        """
        # Convert to grayscale if needed
        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        else:
            gray = image
        
        # Apply denoising
        denoised = cv2.fastNlMeansDenoising(gray)
        
        # Apply adaptive thresholding for better text detection
        adaptive_thresh = cv2.adaptiveThreshold(
            denoised, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
        )
        
        # Apply morphological operations to clean up the image
        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (2, 2))
        cleaned = cv2.morphologyEx(adaptive_thresh, cv2.MORPH_CLOSE, kernel)
        
        return cleaned
    
    def extract_text_tesseract(self, image: np.ndarray) -> Dict[str, Any]:
        """
        Extract text using Tesseract OCR.
        
        Args:
            image: Input image
            
        Returns:
            Dictionary with extracted text and confidence scores
        """
        if not TESSERACT_AVAILABLE:
            return {'text': '', 'confidence': 0, 'method': 'tesseract_unavailable'}
        
        try:
            # Preprocess image
            processed_image = self.preprocess_image(image)
            
            # Build Tesseract language string
            tesseract_langs = []
            for lang in self.languages:
                if lang in OCR_LANGUAGES:
                    tesseract_langs.append(OCR_LANGUAGES[lang])
            
            lang_string = '+'.join(tesseract_langs) if tesseract_langs else 'eng'
            
            # Configure Tesseract
            config = r'--oem 3 --psm 6'
            
            # Extract text
            text = pytesseract.image_to_string(
                processed_image, 
                lang=lang_string, 
                config=config
            ).strip()
            
            # Get confidence scores
            data = pytesseract.image_to_data(processed_image, lang=lang_string, output_type=pytesseract.Output.DICT)
            confidences = [int(conf) for conf in data['conf'] if int(conf) > 0]
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0
            
            return {
                'text': text,
                'confidence': avg_confidence,
                'method': 'tesseract',
                'word_count': len(text.split()) if text else 0
            }
            
        except Exception as e:
            logger.error(f"Tesseract OCR failed: {e}")
            return {'text': '', 'confidence': 0, 'method': 'tesseract_error'}
    
    def extract_text_easyocr(self, image: np.ndarray) -> Dict[str, Any]:
        """
        Extract text using EasyOCR.
        
        Args:
            image: Input image
            
        Returns:
            Dictionary with extracted text and confidence scores
        """
        if not self.easyocr_reader:
            return {'text': '', 'confidence': 0, 'method': 'easyocr_unavailable'}
        
        try:
            # EasyOCR works better with original image
            results = self.easyocr_reader.readtext(image)
            
            # Combine all text and calculate average confidence
            text_parts = []
            confidences = []
            
            for (bbox, text, confidence) in results:
                if confidence > 0.5:  # Filter low-confidence detections
                    text_parts.append(text)
                    confidences.append(confidence)
            
            combined_text = ' '.join(text_parts)
            avg_confidence = sum(confidences) / len(confidences) if confidences else 0
            
            return {
                'text': combined_text,
                'confidence': avg_confidence * 100,  # Convert to percentage
                'method': 'easyocr',
                'word_count': len(combined_text.split()) if combined_text else 0,
                'detections': len(results)
            }
            
        except Exception as e:
            logger.error(f"EasyOCR failed: {e}")
            return {'text': '', 'confidence': 0, 'method': 'easyocr_error'}
    
    def extract_text(self, image: np.ndarray) -> Dict[str, Any]:
        """
        Extract text using the best available OCR method.
        
        Args:
            image: Input image
            
        Returns:
            Dictionary with extracted text and metadata
        """
        results = []
        
        # Try different OCR methods
        if self.ocr_engine in ['tesseract', 'auto'] and TESSERACT_AVAILABLE:
            tesseract_result = self.extract_text_tesseract(image)
            results.append(tesseract_result)
        
        if self.ocr_engine in ['easyocr', 'auto'] and self.easyocr_reader:
            easyocr_result = self.extract_text_easyocr(image)
            results.append(easyocr_result)
        
        if not results:
            return {'text': '', 'confidence': 0, 'method': 'no_ocr_available'}
        
        # Select best result based on confidence and text quality
        best_result = max(results, key=lambda x: x['confidence'])
        
        # Add combined results
        all_text = ' '.join([r['text'] for r in results if r['text']])
        best_result['all_methods_text'] = all_text
        best_result['methods_tried'] = len(results)
        
        return best_result

def _are_frames_similar(frame1, frame2, threshold=0.8):
    """
    Check if two frames are similar using histogram comparison.
    
    Args:
        frame1: First frame
        frame2: Second frame
        threshold: Similarity threshold (0-1)
        
    Returns:
        True if frames are similar, False otherwise
    """
    # Convert to grayscale
    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)
    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)
    
    # Calculate histograms
    hist1 = cv2.calcHist([gray1], [0], None, [256], [0, 256])
    hist2 = cv2.calcHist([gray2], [0], None, [256], [0, 256])
    
    # Compare histograms
    correlation = cv2.compareHist(hist1, hist2, cv2.HISTCMP_CORREL)
    
    return correlation > threshold

def extract_keyframes(
    video_path: str, 
    processed_dir: str, 
    video_id: str,
    target_resolution: str = '480p',
    frame_interval: int = 150,  # Increased interval - check every 5 seconds at 30fps
    similarity_threshold: float = 0.5,  # Reduced threshold to catch more changes
    ocr_languages: List[str] = ['english'],
    enable_ocr: bool = True,
    max_keyframes: int = 20  # Maximum number of keyframes to extract
) -> Optional[str]:
    """
    Extract keyframes from video with OCR text analysis.
    
    Args:
        video_path: Path to input video file
        processed_dir: Directory to save keyframes
        video_id: Unique identifier for the video
        target_resolution: Target resolution for keyframes ('480p', '720p', '1080p')
        frame_interval: Extract one frame every N frames
        similarity_threshold: Threshold for frame similarity (0-1)
        ocr_languages: Languages for OCR text extraction
        enable_ocr: Whether to perform OCR analysis on keyframes
        
    Returns:
        Path to keyframes directory or None if failed
    """
    keyframes_dir = None
    cap = None
    
    try:
        logger.info(f"Extracting keyframes from {video_path}")
        
        # Create output directory
        keyframes_dir = os.path.join(processed_dir, f"{video_id}_keyframes")
        os.makedirs(keyframes_dir, exist_ok=True)
        
        # Initialize OCR if enabled
        ocr_engine = None
        if enable_ocr:
            ocr_engine = KeyframeOCR(languages=ocr_languages)
            logger.info(f"OCR enabled for languages: {ocr_languages}")
        
        # Resolution mapping
        resolution_map = {
            '480p': (854, 480),
            '720p': (1280, 720),
            '1080p': (1920, 1080)
        }
        target_width, target_height = resolution_map.get(target_resolution, (854, 480))
        
        # Open video
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            logger.error(f"Failed to open video: {video_path}")
            return None
        
        # Get video properties
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        logger.info(f"Video FPS: {fps}, Total frames: {total_frames}")
        
        keyframes = []
        keyframe_metadata = []
        prev_frame = None
        frame_count = 0
        saved_count = 0
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            if frame_count % frame_interval == 0:  # Extract every Nth frame
                if saved_count >= max_keyframes:
                    logger.info(f"Reached maximum keyframe limit ({max_keyframes}), stopping extraction")
                    break
                    
                if prev_frame is None or not _are_frames_similar(frame, prev_frame, similarity_threshold):
                    # Resize frame to target resolution
                    resized_frame = cv2.resize(frame, (target_width, target_height))
                    
                    filename = f"keyframe_{saved_count:06d}.jpg"
                    filepath = os.path.join(keyframes_dir, filename)
                    cv2.imwrite(filepath, resized_frame)
                    
                    # Perform OCR if enabled
                    ocr_data = {}
                    if enable_ocr and ocr_engine:
                        ocr_result = ocr_engine.extract_text(resized_frame)
                        ocr_data = {
                            'text': ocr_result.get('text', ''),
                            'confidence': ocr_result.get('confidence', 0),
                            'method': ocr_result.get('method', 'none'),
                            'word_count': ocr_result.get('word_count', 0)
                        }
                    
                    # Store metadata
                    metadata = {
                        'filename': filename,
                        'filepath': filepath,
                        'frame_number': frame_count,
                        'timestamp': frame_count / fps if fps > 0 else 0,
                        'resolution': f"{target_width}x{target_height}",
                        'ocr_data': ocr_data
                    }
                    
                    keyframes.append(filepath)
                    keyframe_metadata.append(metadata)
                    prev_frame = resized_frame.copy()
                    saved_count += 1
            
            frame_count += 1
            
            # Progress logging
            if frame_count % 100 == 0:  # More frequent updates
                progress = (frame_count / total_frames) * 100
                logger.info(f"Progress: {progress:.1f}% - Processed {frame_count}/{total_frames} frames, saved {saved_count} keyframes")
                
                # Early stopping if we have enough keyframes
                if saved_count >= 50:  # Limit to 50 keyframes for reasonable processing time
                    logger.info("Reached maximum keyframe limit, stopping extraction")
                    break
        
        # Save metadata
        metadata_file = os.path.join(keyframes_dir, 'keyframes_metadata.json')
        try:
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'video_id': video_id,
                    'total_keyframes': len(keyframes),
                    'target_resolution': target_resolution,
                    'frame_interval': frame_interval,
                    'similarity_threshold': similarity_threshold,
                    'ocr_enabled': enable_ocr,
                    'ocr_languages': ocr_languages,
                    'keyframes': keyframe_metadata
                }, f, indent=2, ensure_ascii=False)
            logger.info(f"Keyframe metadata saved to {metadata_file}")
        except Exception as e:
            logger.error(f"Failed to save keyframe metadata: {e}")
        
        logger.info(f"Extracted {len(keyframes)} keyframes to {keyframes_dir}")
        
        # Extract and summarize OCR text
        if enable_ocr:
            all_ocr_text = []
            high_confidence_text = []
            
            for metadata in keyframe_metadata:
                ocr_data = metadata.get('ocr_data', {})
                text = ocr_data.get('text', '')
                confidence = ocr_data.get('confidence', 0)
                
                if text:
                    all_ocr_text.append(text)
                    if confidence > 70:  # High confidence threshold
                        high_confidence_text.append(text)
            
            # Save OCR summary
            ocr_summary = {
                'total_text_frames': len([t for t in all_ocr_text if t]),
                'high_confidence_frames': len(high_confidence_text),
                'all_text': ' '.join(all_ocr_text),
                'high_confidence_text': ' '.join(high_confidence_text),
                'word_frequency': dict(Counter(' '.join(all_ocr_text).split()).most_common(20))
            }
            
            ocr_summary_file = os.path.join(keyframes_dir, 'ocr_summary.json')
            try:
                with open(ocr_summary_file, 'w', encoding='utf-8') as f:
                    json.dump(ocr_summary, f, indent=2, ensure_ascii=False)
                logger.info(f"OCR summary saved to {ocr_summary_file}")
            except Exception as e:
                logger.error(f"Failed to save OCR summary: {e}")
        
        return keyframes_dir
        
    except Exception as e:
        logger.error(f"Error extracting keyframes: {e}")
        return None
        
    finally:
        if cap is not None:
            cap.release()

def get_keyframe_text_summary(keyframes_dir: str) -> Dict[str, Any]:
    """
    Get OCR text summary for keyframes.
    
    Args:
        keyframes_dir: Directory containing keyframes
        
    Returns:
        Dictionary with OCR text summary
    """
    ocr_summary_file = os.path.join(keyframes_dir, 'ocr_summary.json')
    
    if os.path.exists(ocr_summary_file):
        try:
            with open(ocr_summary_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"Failed to load OCR summary: {e}")
    
    return {
        'total_text_frames': 0,
        'high_confidence_frames': 0,
        'all_text': '',
        'high_confidence_text': '',
        'word_frequency': {}
    }
</file>

<file path="utils/transcriber.py">
import os
import json
import logging
import whisper
import torch
from typing import Dict, List, Tuple, Optional
from pydub import AudioSegment
import subprocess

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# All major Indian languages supported by Whisper
# Based on the 22 official languages of India as per the Constitution
SUPPORTED_LANGUAGES = {
    # Major Indian Languages
    'hindi': 'hi',           # ‡§π‡§ø‡§®‡•ç‡§¶‡•Ä - Most widely spoken
    'bengali': 'bn',         # ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ - 2nd most spoken
    'telugu': 'te',          # ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å - Andhra Pradesh, Telangana
    'marathi': 'mr',         # ‡§Æ‡§∞‡§æ‡§†‡•Ä - Maharashtra
    'tamil': 'ta',           # ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç - Tamil Nadu
    'gujarati': 'gu',        # ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä - Gujarat
    'urdu': 'ur',            # ÿßÿ±ÿØŸà - Widely understood
    'kannada': 'kn',         # ‡≤ï‡≤®‡≥ç‡≤®‡≤° - Karnataka
    'odia': 'or',            # ‡¨ì‡¨°‡¨º‡¨ø‡¨Ü - Odisha
    'malayalam': 'ml',       # ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç - Kerala
    'punjabi': 'pa',         # ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä - Punjab
    'assamese': 'as',        # ‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ - Assam
    'maithili': 'mai',       # ‡§Æ‡•à‡§•‡§ø‡§≤‡•Ä - Bihar, Nepal
    'santali': 'sat',        # ·±•·±ü·±±·±õ·±ü·±≤·±§ - Jharkhand, West Bengal
    'nepali': 'ne',          # ‡§®‡•á‡§™‡§æ‡§≤‡•Ä - Sikkim, West Bengal
    'kashmiri': 'ks',        # ‡§ï‡•â‡§∂‡•Å‡§∞ / ⁄©Ÿ≤ÿ¥Ÿèÿ± - Kashmir
    'konkani': 'gom',        # ‡§ï‡•ã‡§Ç‡§ï‡§£‡•Ä - Goa
    'sindhi': 'sd',          # ÿ≥ŸÜÿØ⁄æ€å / ‡§∏‡§ø‡§®‡•ç‡§ß‡•Ä - 
    'dogri': 'doi',          # ‡§°‡•ã‡§ó‡§∞‡•Ä - Jammu & Kashmir
    'manipuri': 'mni',       # ‡¶Æ‡ßà‡¶á‡¶§‡ßà‡¶á‡¶≤‡ßã‡¶®‡ßç - Manipur
    'bodo': 'brx',           # ‡§¨‡§∞'/‡§¨‡§°‡§º‡•ã - Assam
    'sanskrit': 'sa',        # ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§ - Classical language
    
    # International languages for comparison/translation
    'english': 'en',         # English - Widely used
    'arabic': 'ar',          # ÿßŸÑÿπÿ±ÿ®Ÿäÿ© - For Islamic content
    'chinese': 'zh',         # ‰∏≠Êñá - For international content
    'spanish': 'es',         # Espa√±ol - International
    'french': 'fr',          # Fran√ßais - International
    'german': 'de',          # Deutsch - International
    'japanese': 'ja',        # Êó•Êú¨Ë™û - International
    'korean': 'ko',          # ÌïúÍµ≠Ïñ¥ - International
    'russian': 'ru',         # –†—É—Å—Å–∫–∏–π - International
    'portuguese': 'pt',      # Portugu√™s - International
}

# For backward compatibility
LANGUAGE_MAP = SUPPORTED_LANGUAGES

# Model size mapping for different languages
# Larger models are more accurate but slower and require more memory
# Recommendations based on language complexity and available training data
MODEL_SIZES = {
    # Well-supported languages with large datasets
    'en': 'base',       # English - excellent support
    'hi': 'medium',     # Hindi - good support
    'bn': 'medium',     # Bengali - good support
    'ta': 'medium',     # Tamil - good support
    'te': 'medium',     # Telugu - good support
    'mr': 'medium',     # Marathi - good support
    'gu': 'medium',     # Gujarati - good support
    'ur': 'medium',     # Urdu - good support
    'kn': 'medium',     # Kannada - good support
    'ml': 'medium',     # Malayalam - good support
    'pa': 'medium',     # Punjabi - good support
    
    # Languages with moderate support - use larger model for better accuracy
    'or': 'large',      # Odia - moderate support
    'as': 'large',      # Assamese - moderate support
    'mai': 'large',     # Maithili - limited support
    'ne': 'large',      # Nepali - moderate support
    'sa': 'large',      # Sanskrit - specialized
    
    # Languages with limited support - use largest model available
    'sat': 'large',     # Santali - limited support
    'ks': 'large',      # Kashmiri - limited support
    'gom': 'large',     # Konkani - limited support
    'sd': 'large',      # Sindhi - limited support
    'doi': 'large',     # Dogri - very limited support
    'mni': 'large',     # Manipuri - limited support
    'brx': 'large',     # Bodo - limited support
    
    # International languages
    'ar': 'medium',     # Arabic - good support
    'zh': 'medium',     # Chinese - good support
    'es': 'base',       # Spanish - excellent support
    'fr': 'base',       # French - excellent support
    'de': 'base',       # German - excellent support
    'ja': 'medium',     # Japanese - good support
    'ko': 'medium',     # Korean - good support
    'ru': 'medium',     # Russian - good support
    'pt': 'base',       # Portuguese - good support
}

def extract_audio(
    video_path: str, 
    output_path: str, 
    sample_rate: int = 16000
) -> bool:
    """Extract audio from video using ffmpeg.
    
    Args:
        video_path: Path to input video file
        output_path: Path to save extracted audio
        sample_rate: Sample rate for output audio (Hz)
        
    Returns:
        bool: True if successful, False otherwise
    """
    try:
        cmd = [
            'ffmpeg',
            '-y',  # Overwrite output file if it exists
            '-i', video_path,
            '-vn',  # Disable video
            '-acodec', 'pcm_s16le',  # 16-bit PCM
            '-ar', str(sample_rate),  # Sample rate
            '-ac', '1',  # Mono audio
            '-f', 'wav',
            output_path
        ]
        
        result = subprocess.run(
            cmd, 
            check=True, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.PIPE,
            text=True
        )
        logger.debug(f"FFmpeg audio extraction output: {result.stdout}")
        return True
        
    except subprocess.CalledProcessError as e:
        logger.error(f"Error extracting audio: {e.stderr}")
        return False
    except Exception as e:
        logger.error(f"Unexpected error in extract_audio: {str(e)}", exc_info=True)
        return False

def transcribe_video(
    video_path: str, 
    processed_dir: str, 
    video_id: str, 
    language: str = 'english',
    model_size: Optional[str] = None
) -> Tuple[Optional[str], List[Dict]]:
    """Transcribe video to text using Whisper.
    
    Args:
        video_path: Path to input video file
        processed_dir: Directory to save output files
        video_id: Unique ID for the video
        language: Language of the video ('english', 'hindi', or 'kannada')
        model_size: Override the default model size (tiny, base, small, medium, large)
        
    Returns:
        Tuple containing:
        - Path to segments JSON file (or None if failed)
        - List of segment dictionaries
    """
    try:
        # Validate language
        language = language.lower()
        if language not in LANGUAGE_MAP:
            logger.warning(f"Unsupported language: {language}. Defaulting to English.")
            language = 'english'
            
        lang_code = LANGUAGE_MAP[language]
        
        # Set model size if not provided
        if model_size is None:
            model_size = MODEL_SIZES.get(lang_code, 'base')
        
        logger.info(f"Transcribing video in {language} using Whisper {model_size} model...")
        
        # Create output directory if it doesn't exist
        os.makedirs(processed_dir, exist_ok=True)
        
        # Paths for intermediate and output files
        audio_path = os.path.join(processed_dir, f"{video_id}_audio.wav")
        seg_path = os.path.join(processed_dir, f"{video_id}_segments.json")
        
        # Extract audio from video
        if not extract_audio(video_path, audio_path):
            logger.error("Failed to extract audio from video")
            return None, []
        
        # Check if GPU is available
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        logger.info(f"Using device: {device}")
        
        # Load Whisper model
        model = whisper.load_model(model_size, device=device)
        
        # Transcribe audio
        result = model.transcribe(
            audio_path,
            language=lang_code,
            verbose=True,
            fp16=(device == 'cuda')  # Use mixed precision on GPU
        )
        
        # Get segments
        segments = result.get('segments', [])
        
        # Save segments to file
        with open(seg_path, 'w', encoding='utf-8') as f:
            json.dump(segments, f, indent=2, ensure_ascii=False)
        
        logger.info(f"Transcription complete. Saved to {seg_path}")
        return seg_path, segments
        
    except Exception as e:
        logger.error(f"Error in transcribe_video: {str(e)}", exc_info=True)
        return None, []
    finally:
        # Clean up temporary audio file if it exists
        if 'audio_path' in locals() and os.path.exists(audio_path):
            try:
                os.remove(audio_path)
            except Exception as e:
                logger.warning(f"Failed to remove temporary audio file: {e}")
</file>

<file path="utils/tts.py">
import os
import logging
from gtts import gTTS
from pydub import AudioSegment
from typing import Optional, Dict, Tuple
import tempfile
import time
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Language mappings for TTS
LANGUAGE_MAP = {
    # Major Indian Languages
    'hindi': 'hi',
    'bengali': 'bn',
    'telugu': 'te',
    'marathi': 'mr',
    'tamil': 'ta',
    'gujarati': 'gu',
    'urdu': 'ur',
    'kannada': 'kn',
    'odia': 'or',
    'malayalam': 'ml',
    'punjabi': 'pa',
    'assamese': 'as',
    'sanskrit': 'sa',
    
    # International languages
    'english': 'en',
    'arabic': 'ar',
    'chinese': 'zh',
    'spanish': 'es',
    'french': 'fr',
    'german': 'de',
    'japanese': 'ja',
    'korean': 'ko',
    'russian': 'ru'
}

# Supported voices configuration
SUPPORTED_VOICES = {
    'en': [
        {'id': 'en-US-Standard-A', 'name': 'English US (Female)'},
        {'id': 'en-US-Standard-B', 'name': 'English US (Male)'},
        {'id': 'en-GB-Standard-A', 'name': 'English UK (Female)'},
        {'id': 'en-GB-Standard-B', 'name': 'English UK (Male)'}
    ],
    'hi': [
        {'id': 'hi-IN-Standard-A', 'name': 'Hindi (Female)'},
        {'id': 'hi-IN-Standard-B', 'name': 'Hindi (Male)'}
    ],
    'bn': [
        {'id': 'bn-IN-Standard-A', 'name': 'Bengali (Female)'},
        {'id': 'bn-IN-Standard-B', 'name': 'Bengali (Male)'}
    ],
    'te': [
        {'id': 'te-IN-Standard-A', 'name': 'Telugu (Female)'},
        {'id': 'te-IN-Standard-B', 'name': 'Telugu (Male)'}
    ],
    'ta': [
        {'id': 'ta-IN-Standard-A', 'name': 'Tamil (Female)'},
        {'id': 'ta-IN-Standard-B', 'name': 'Tamil (Male)'}
    ]
}

# Voice settings for gTTS
VOICE_SETTINGS = {
    # Indian Languages
    'hi': {'tld': 'co.in', 'slow': False, 'lang': 'hi'},
    'bn': {'tld': 'co.in', 'slow': False, 'lang': 'bn'},
    'te': {'tld': 'co.in', 'slow': False, 'lang': 'te'},
    'mr': {'tld': 'co.in', 'slow': False, 'lang': 'mr'},
    'ta': {'tld': 'co.in', 'slow': False, 'lang': 'ta'},
    'gu': {'tld': 'co.in', 'slow': False, 'lang': 'gu'},
    'ur': {'tld': 'co.in', 'slow': False, 'lang': 'ur'},
    'kn': {'tld': 'co.in', 'slow': False, 'lang': 'kn'},
    'or': {'tld': 'co.in', 'slow': False, 'lang': 'or'},
    'ml': {'tld': 'co.in', 'slow': False, 'lang': 'ml'},
    'pa': {'tld': 'co.in', 'slow': False, 'lang': 'pa'},
    'as': {'tld': 'co.in', 'slow': False, 'lang': 'as'},
    'sa': {'tld': 'co.in', 'slow': True, 'lang': 'sa'},
    
    # International Languages
    'en': {'tld': 'com', 'slow': False, 'lang': 'en'},
    'ar': {'tld': 'com', 'slow': False, 'lang': 'ar'},
    'zh': {'tld': 'com', 'slow': False, 'lang': 'zh'},
    'es': {'tld': 'com', 'slow': False, 'lang': 'es'},
    'fr': {'tld': 'fr', 'slow': False, 'lang': 'fr'},
    'de': {'tld': 'de', 'slow': False, 'lang': 'de'},
    'ja': {'tld': 'co.jp', 'slow': False, 'lang': 'ja'},
    'ko': {'tld': 'co.kr', 'slow': False, 'lang': 'ko'},
    'ru': {'tld': 'ru', 'slow': False, 'lang': 'ru'}
}

def text_to_speech(
    text: str, 
    output_dir: str, 
    video_id: str, 
    cluster_id: int,
    voice: str = 'en-US-Standard-C',
    language: str = 'english',
    slow: bool = False,
    bitrate: str = '192k',
    max_retries: int = 3,
    retry_delay: int = 2
) -> Optional[str]:
    """Convert text to speech using gTTS with retry logic and offline fallback.
    
    Args:
        text: Text to convert to speech
        output_dir: Directory to save the output file
        video_id: Unique ID for the video
        cluster_id: ID of the current topic cluster
        voice: Voice ID to use (for compatibility with advanced TTS)
        language: Language of the text (supports all Indian languages)
        slow: Whether to speak slowly (better for some languages)
        bitrate: Audio bitrate (e.g., '128k', '192k', '256k')
        max_retries: Maximum number of retry attempts for network issues
        retry_delay: Delay in seconds between retries
        
    Returns:
        Relative path to the generated audio file, or None on failure
    """
    if not text.strip():
        logger.warning("Empty text provided for TTS")
        return None
    
    temp_mp3_path = None
    attempt = 0
    
    while attempt < max_retries:
        try:
            # Validate language
            language = language.lower()
            if language not in LANGUAGE_MAP:
                logger.warning(f"Unsupported language: {language}. Defaulting to English.")
                language = 'english'
                
            lang_code = LANGUAGE_MAP[language]
            voice_settings = VOICE_SETTINGS.get(lang_code, VOICE_SETTINGS['en']).copy()
            voice_settings['slow'] = slow
            
            logger.info(f"Generating speech in {language} (Attempt {attempt + 1}/{max_retries})...")
            
            # Create output directory if it doesn't exist
            os.makedirs(output_dir, exist_ok=True)
            
            # Generate a temporary file path
            with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as temp_mp3:
                temp_mp3_path = temp_mp3.name
            
            # Generate speech using gTTS
            tts = gTTS(
                text=text,
                lang=voice_settings['lang'],
                tld=voice_settings['tld'],
                slow=voice_settings['slow']
            )
            
            # Save as MP3 first (gTTS works better with MP3)
            tts.save(temp_mp3_path)
            
            # Output file path (WAV format for better compatibility)
            output_file = os.path.join(output_dir, f"{video_id}_summary_{cluster_id}.wav")
            
            # Convert to WAV with pydub for better control over format
            audio = AudioSegment.from_mp3(temp_mp3_path)
            
            # Normalize audio volume
            audio = audio.normalize()
            
            # Export as WAV with specified bitrate
            audio.export(
                output_file,
                format='wav',
                parameters=['-ar', '44100', '-ac', '2', '-b:a', bitrate]
            )
            
            logger.info(f"Speech generated and saved to {output_file}")
            return output_file
            
        except Exception as e:
            attempt += 1
            logger.error(f"TTS attempt {attempt} failed: {str(e)}")
            
            if attempt < max_retries:
                logger.info(f"Retrying in {retry_delay} seconds...")
                time.sleep(retry_delay)
                # Increase delay for next attempt
                retry_delay *= 2
            else:
                logger.error("All TTS attempts failed. Trying offline fallback...")
                # Here you could implement an offline TTS fallback
                # For now, we'll return None to indicate failure
                return None
        
        finally:
            # Clean up temporary files
            if temp_mp3_path and os.path.exists(temp_mp3_path):
                try:
                    os.remove(temp_mp3_path)
                except Exception as e:
                    logger.warning(f"Failed to remove temporary MP3 file: {e}")

def get_available_voices():
    """Get a dictionary of available voices organized by language.
    
    Returns:
        dict: A dictionary mapping language codes to lists of available voices
    """
    return SUPPORTED_VOICES

def get_available_languages():
    """Get a dictionary of available languages and their codes."""
    return LANGUAGE_MAP.copy()
</file>

<file path="utils/video_maker.py">
import os
import cv2
import numpy as np
from pydub import AudioSegment
import subprocess
import logging
import shutil
from typing import Optional, Tuple, Dict, List
from pathlib import Path

# Supported video resolutions (width, height)
SUPPORTED_RESOLUTIONS = {
    '360p': (640, 360),
    '480p': (854, 480),
    '720p': (1280, 720),
    '1080p': (1920, 1080)
}

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def check_openh264():
    """Check and download OpenH264 if needed."""
    try:
        # Check if OpenH264 is in the current directory
        openh264_path = Path("openh264-1.8.0-win64.dll")
        if not openh264_path.exists():
            logger.info("OpenH264 not found. Attempting to download...")
            import urllib.request
            url = "https://github.com/cisco/openh264/releases/download/v1.8.0/openh264-1.8.0-win64.dll"
            urllib.request.urlretrieve(url, openh264_path)
            logger.info("OpenH264 downloaded successfully")
        return True
    except Exception as e:
        logger.error(f"Failed to download OpenH264: {e}")
        return False

def resize_frame(frame: np.ndarray, target_width: int = 854) -> np.ndarray:
    """Resize frame to target width while maintaining aspect ratio."""
    height, width = frame.shape[:2]
    aspect_ratio = width / height
    target_height = int(target_width / aspect_ratio)
    return cv2.resize(frame, (target_width, target_height), interpolation=cv2.INTER_LINEAR)

def add_subtitle_to_frame(frame: np.ndarray, text: str, position: Tuple[int, int] = (50, 50), 
                         font_scale: float = 1.0, color: Tuple[int, int, int] = (255, 255, 255),
                         thickness: int = 2, font_face: int = cv2.FONT_HERSHEY_SIMPLEX) -> np.ndarray:
    """Add subtitle text to a video frame."""
    frame = frame.copy()  # Create a copy to avoid modifying the original
    
    # Split text into multiple lines if too long
    max_width = frame.shape[1] - 100  # Leave margins
    words = text.split()
    lines = []
    current_line = []
    
    for word in words:
        current_line.append(word)
        (text_width, _), _ = cv2.getTextSize(' '.join(current_line), font_face, font_scale, thickness)
        if text_width > max_width:
            if len(current_line) > 1:
                current_line.pop()
                lines.append(' '.join(current_line))
                current_line = [word]
            else:
                lines.append(' '.join(current_line))
                current_line = []
    
    if current_line:
        lines.append(' '.join(current_line))
    
    # Draw each line
    x, y = position
    line_height = int(text_height * 1.5)
    
    for i, line in enumerate(lines):
        y_pos = y + i * line_height
        (text_width, text_height), _ = cv2.getTextSize(line, font_face, font_scale, thickness)
        
        # Add black background for better text visibility
        cv2.rectangle(frame, 
                     (x-5, y_pos - text_height - 5),
                     (x + text_width + 5, y_pos + 5),
                     (0, 0, 0), -1)
        
        # Add text
        cv2.putText(frame, line, (x, y_pos), font_face, font_scale, color, thickness, cv2.LINE_AA)
    
    return frame

def make_summary_video(
    keyframes_dir: str, 
    tts_audio_relpath: str, 
    processed_dir: str, 
    video_id: str, 
    cluster_id: int,
    subtitles: Optional[list] = None,
    target_width: int = 854,
    fps: int = 30
) -> Optional[str]:
    """
    Create a summary video from keyframes and audio.
    
    Args:
        keyframes_dir: Directory containing keyframe images
        tts_audio_relpath: Path to TTS audio file (relative to processed_dir)
        processed_dir: Directory to save output files
        video_id: Unique ID for the video
        cluster_id: ID of the current topic cluster
        subtitles: List of (start_time, end_time, text) for subtitles
        target_width: Target width of output video (height will be calculated)
        fps: Frames per second for output video
        
    Returns:
        Relative path to the generated video file, or None on failure
    """
    temp_video = None
    temp_audio = None
    
    try:
        # Ensure OpenH264 is available
        if not check_openh264():
            logger.error("OpenH264 not available. Video creation may fail.")
        
        # Get all image files
        img_files = sorted(
            [f for f in os.listdir(keyframes_dir) 
             if f.lower().endswith(('.png', '.jpg', '.jpeg'))],
            key=lambda x: int(''.join(filter(str.isdigit, x)) or 0)
        )
        
        if not img_files:
            logger.error(f"No image files found in {keyframes_dir}")
            return None
        
        # Read the first image to get dimensions
        first_img = cv2.imread(os.path.join(keyframes_dir, img_files[0]))
        if first_img is None:
            logger.error(f"Failed to read image: {img_files[0]}")
            return None
            
        # Resize to target dimensions while maintaining aspect ratio
        first_img = resize_frame(first_img, target_width)
        height, width = first_img.shape[:2]
        
        # Create processed directory if it doesn't exist
        os.makedirs(processed_dir, exist_ok=True)
        
        # Output paths
        temp_video = os.path.join(processed_dir, f"{video_id}_summary_{cluster_id}_temp.mp4")
        final_video = os.path.join(processed_dir, f"{video_id}_summary_{cluster_id}.mp4")
        
        # Create video writer with better quality settings
        fourcc = cv2.VideoWriter_fourcc(*'avc1')  # Better compatibility than 'mp4v'
        out = cv2.VideoWriter(
            temp_video, 
            fourcc, 
            fps,
            (width, height)
        )
        
        if not out.isOpened():
            logger.error("Failed to create video writer. Trying alternative codec...")
            # Try alternative codec
            out = cv2.VideoWriter(
                temp_video,
                cv2.VideoWriter_fourcc(*'XVID'),
                fps,
                (width, height)
            )
            if not out.isOpened():
                raise Exception("Failed to initialize video writer with both codecs")
        
        # Load and process audio
        audio_file = os.path.join(processed_dir, tts_audio_relpath)
        if not os.path.exists(audio_file):
            logger.error(f"Audio file not found: {audio_file}")
            return None
            
        audio = AudioSegment.from_file(audio_file)
        audio_duration = len(audio) / 1000.0  # Convert to seconds
        
        # Calculate how many times to repeat each frame to match audio duration
        if len(img_files) > 0:
            frames_per_image = max(1, int((audio_duration * fps) / len(img_files)))
        else:
            frames_per_image = fps  # Fallback: 1 second per frame
        
        # Process each frame
        frame_times = []  # For subtitle timing
        current_time = 0
        time_per_frame = 1.0 / fps
        
        for img_file in img_files:
            img_path = os.path.join(keyframes_dir, img_file)
            frame = cv2.imread(img_path)
            if frame is None:
                logger.warning(f"Skipping corrupted image: {img_file}")
                continue
                
            # Resize frame
            frame = resize_frame(frame, target_width)
            
            # Add this frame multiple times to match audio duration
            for _ in range(frames_per_image):
                # Add subtitles if available
                if subtitles:
                    # Find current subtitle
                    current_subtitle = next(
                        (sub for sub in subtitles 
                         if sub[0] <= current_time < sub[1]),
                        None
                    )
                    if current_subtitle:
                        frame = add_subtitle_to_frame(
                            frame,
                            current_subtitle[2],  # Subtitle text
                            (50, height - 50)     # Position at bottom
                        )
                
                out.write(frame)
                frame_times.append(current_time)
                current_time += time_per_frame
        
        # Release the video writer
        out.release()
        
        # Trim audio to match video duration if needed
        video_duration = len(frame_times) * time_per_frame
        if audio_duration > video_duration:
            audio = audio[:int(video_duration * 1000)]
        
        # Save the trimmed audio
        temp_audio = os.path.join(processed_dir, f"temp_audio_{video_id}_{cluster_id}.wav")
        audio.export(temp_audio, format="wav")
        
        # Combine video and audio using ffmpeg with better quality settings
        cmd = [
            'ffmpeg',
            '-y',  # Overwrite output file if it exists
            '-i', temp_video,  # Input video
            '-i', temp_audio,   # Input audio
            '-c:v', 'libx264',
            '-profile:v', 'main',
            '-preset', 'medium',
            '-crf', '23',  # Constant Rate Factor (lower = better quality, 23 is default)
            '-pix_fmt', 'yuv420p',  # Better compatibility
            '-c:a', 'aac',
            '-b:a', '192k',  # Higher audio bitrate
            '-shortest',
            '-movflags', '+faststart',  # Enable streaming
            final_video
        ]
        
        try:
            result = subprocess.run(
                cmd, 
                check=True, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE,
                text=True
            )
            logger.debug(f"FFmpeg output: {result.stdout}")
            
            if not os.path.exists(final_video):
                logger.error(f"Output video not created: {final_video}")
                return None
                
            logger.info(f"Successfully created summary video: {final_video}")
            return os.path.relpath(final_video, processed_dir)
            
        except subprocess.CalledProcessError as e:
            logger.error(f"FFmpeg error: {e.stderr}")
            return None
            
    except Exception as e:
        logger.error(f"Error in make_summary_video: {str(e)}", exc_info=True)
        return None
        
    finally:
        # Clean up temporary files
        for temp_file in [temp_video, temp_audio]:
            try:
                if temp_file and os.path.exists(temp_file):
                    os.remove(temp_file)
            except Exception as e:
                logger.warning(f"Failed to remove temp file {temp_file}: {e}")
</file>

<file path="app.py">
import os
import json
import time
import logging
import traceback
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union
from concurrent.futures import ThreadPoolExecutor
import re
from flask import (
    Flask, render_template, request, jsonify, 
    send_from_directory, session, Response, stream_with_context
)
from werkzeug.utils import secure_filename

# Import utility modules
from utils.downloader import handle_video_upload_or_download, is_youtube_url
from utils.transcriber import transcribe_video, SUPPORTED_LANGUAGES as TRANSCRIBE_LANGS
from utils.keyframes import extract_keyframes, get_keyframe_text_summary
from utils.clustering import cluster_segments, extract_keywords
from utils.summarizer import summarize_cluster
from utils.tts import text_to_speech, SUPPORTED_VOICES, get_available_voices
from utils.video_maker import make_summary_video, SUPPORTED_RESOLUTIONS
from utils.translator import (
    translate_text, translate_segments, get_available_languages,
    detect_language, LANGUAGE_MAPPINGS
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(filename='app.log',encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# Initialize Flask app
app = Flask(__name__)
app.secret_key = os.environ.get('FLASK_SECRET_KEY', 'dev-key-change-in-production')

# Configuration
CONFIG = {
    'UPLOAD_DIR': 'uploads',
    'PROCESSED_DIR': 'processed',
    'MAX_CONTENT_LENGTH': 40 * 1024 * 1024,  # 40MB
    'MAX_VIDEO_DURATION': 40 * 60,  # 40 minutes in seconds
    'SUPPORTED_EXTENSIONS': {'mp4', 'avi', 'mov', 'mkv', 'webm'},
    'DEFAULT_LANGUAGE': 'en',
    'DEFAULT_VOICE': 'en-US-Standard-C',
    'DEFAULT_RESOLUTION': '480p',
    'ENABLE_DARK_MODE': True,
}
# Ensure directories exist
for dir_path in [CONFIG['UPLOAD_DIR'], CONFIG['PROCESSED_DIR']]:
    os.makedirs(dir_path, exist_ok=True)

# Global state for tracking processing status
processing_status = {}

def get_processing_status(video_id: str) -> Dict:
    """Get the current processing status for a video."""
    return processing_status.get(video_id, {
        'status': 'not_started',
        'progress': 0,
        'message': 'Processing not started',
        'error': None
    })

def update_processing_status(video_id: str, status: str, progress: int, message: str, error: str = None) -> None:
    """Update the processing status for a video."""
    if video_id not in processing_status:
        processing_status[video_id] = {}
    
    processing_status[video_id].update({
        'status': status,
        'progress': progress,
        'message': message,
        'error': error,
        'last_updated': time.time()
    })

@app.route('/')
def index():
    """Render the main page with language and voice options."""
    # Get available voices grouped by language
    voices_by_lang = {}
    available_voices = get_available_voices()
    
    # voices_by_lang will have the structure: {lang_code: {'name': str, 'voices': list}}
    for lang_code, voices in available_voices.items():
        voices_by_lang[lang_code] = {
            'name': TRANSCRIBE_LANGS.get(lang_code, 'Unknown'),
            'voices': voices
        }
    
    # Sort languages by name
    sorted_languages = sorted(
        [(code, data['name']) for code, data in voices_by_lang.items()],
        key=lambda x: x[1]
    )
    
    # Get default language and voice
    default_lang = request.cookies.get('preferred_language', CONFIG['DEFAULT_LANGUAGE'])
    default_voice = request.cookies.get('preferred_voice', CONFIG['DEFAULT_VOICE'])
    dark_mode = request.cookies.get('dark_mode', 'true') == 'true'
    
    return render_template(
        'index.html',
        languages=sorted_languages,
        voices_by_lang=voices_by_lang,
        default_language=default_lang,
        default_voice=default_voice,
        dark_mode=dark_mode,
        max_file_size=CONFIG['MAX_CONTENT_LENGTH'],
        max_duration=CONFIG['MAX_VIDEO_DURATION'],
        supported_extensions=', '.join(CONFIG['SUPPORTED_EXTENSIONS']),
        resolutions=SUPPORTED_RESOLUTIONS,
        default_resolution=CONFIG['DEFAULT_RESOLUTION']
    )
from flask import copy_current_request_context
@app.route('/api/process', methods=['POST'])
def process():
    """Handle video processing request with enhanced language support."""
    # Get request data
    video_url = request.form.get('yt_url', '').strip()
    source_language = request.form.get('source_language', 'auto')
    target_language = request.form.get('target_language', 'english')
    voice = request.form.get('voice', CONFIG['DEFAULT_VOICE'])
    resolution = request.form.get('resolution', CONFIG['DEFAULT_RESOLUTION'])
    summary_length = request.form.get('summary_length', 'medium')
    enable_ocr = request.form.get('enable_ocr', 'true').lower() == 'true'
    
    # Validate languages
    if source_language != 'auto' and source_language not in TRANSCRIBE_LANGS:
        return jsonify({'error': f'Unsupported source language: {source_language}'}), 400
    
    if target_language not in LANGUAGE_MAPPINGS:
        return jsonify({'error': f'Unsupported target language: {target_language}'}), 400
    
    # Generate a unique ID for this processing job
    video_id= str(int(time.time()))
    @copy_current_request_context
    # Start processing in background
    def process_video():
        nonlocal video_id
        try:
            update_processing_status(video_id, 'downloading', 5, 'Downloading video...')
            
            # 1. Download or save the uploaded video
            video_path, actual_video_id = handle_video_upload_or_download(
                request, 
                CONFIG['UPLOAD_DIR']
            )
            
            # Use the actual video ID from the handler
            video_id = actual_video_id
            
            # 2. Transcribe the video
            update_processing_status(video_id, 'transcribing', 20, 'Transcribing audio...')
            
            # Use auto-detection if source language is auto
            transcription_language = source_language if source_language != 'auto' else 'english'
            
            transcript_path, segments = transcribe_video(
                video_path, 
                CONFIG['PROCESSED_DIR'], 
                video_id,
                language=transcription_language
            )
            
            if not segments:
                raise ValueError("No speech detected in the video")
                
            # Auto-detect language if needed
            detected_language = transcription_language
            if source_language == 'auto' and segments:
                sample_text = ' '.join([seg.get('text', '') for seg in segments[:5]])
                detected_lang = detect_language(sample_text)
                if detected_lang:
                    detected_language = detected_lang
                    logger.info(f"Auto-detected language: {detected_language}")
            
            # Translate segments if source and target languages are different
            if detected_language != target_language:
                update_processing_status(video_id, 'translating', 35, 'Translating content...')
                segments = translate_segments(segments, detected_language, target_language)
                logger.info(f"Translated from {detected_language} to {target_language}")
            
            # 3. Extract key frames with OCR
            update_processing_status(video_id, 'extracting', 40, 'Extracting key frames...')
            
            # Prepare OCR languages
            ocr_languages = [detected_language, target_language]
            if 'english' not in ocr_languages:
                ocr_languages.append('english')
            
            keyframes_dir = extract_keyframes(
                video_path, 
                CONFIG['PROCESSED_DIR'], 
                video_id,
                target_resolution=resolution,
                ocr_languages=ocr_languages,
                enable_ocr=enable_ocr
            )
            
            # Get OCR text summary for additional context
            ocr_summary = {}
            if enable_ocr and keyframes_dir:
                ocr_summary = get_keyframe_text_summary(keyframes_dir)
                if ocr_summary.get('high_confidence_text'):
                    logger.info(f"Extracted OCR text: {len(ocr_summary['high_confidence_text'])} characters")
            
            # 4. Cluster transcript into topics
            update_processing_status(video_id, 'clustering', 55, 'Analyzing content...')
            
            # Enhance segments with OCR context if available
            if enable_ocr and ocr_summary.get('high_confidence_text'):
                # Add OCR context to the first few segments for better clustering
                ocr_context = ocr_summary['high_confidence_text'][:500]  # Limit context
                if segments and ocr_context.strip():
                    segments[0]['text'] += f" [Visual context: {ocr_context}]"
            
            clustered = cluster_segments(
                segments,
                language=target_language,  # Use target language for clustering
                method='lda',  # or 'nmf', 'kmeans', 'dbscan'
                n_clusters=min(5, max(2, len(segments) // 10))  # Dynamic cluster count
            )
            
            # 5. Process each cluster
            summaries = []
            tts_paths = []
            cluster_keywords = []
            
            for cluster_id, cluster in enumerate(clustered):
                # Get keywords for this cluster
                cluster_texts = [seg['text'] for seg in cluster]
                keywords = extract_keywords(
                    cluster_texts, 
                    n_keywords=5,
                    language=target_language
                )
                cluster_keywords.append(keywords)
                
                # Generate summary in target language
                update_processing_status(
                    video_id, 
                    'summarizing', 
                    60 + (20 * cluster_id // len(clustered)),
                    f'Generating summary for topic {cluster_id + 1}...'
                )
                summary = summarize_cluster(
                    cluster,
                    language=target_language,
                    use_extractive=True
                )
                summaries.append(summary)
                
                # Generate TTS audio in target language
                tts_path = text_to_speech(
                    text=summary,
                    output_dir=CONFIG['PROCESSED_DIR'],
                    video_id=video_id,
                    cluster_id=cluster_id,
                    voice=voice,
                    language=target_language
                )
                tts_paths.append(tts_path)
            
            # 6. Create summary videos
            summary_videos = []
            for cluster_id, (tts_path, keywords) in enumerate(zip(tts_paths, cluster_keywords)):
                update_processing_status(
                    video_id,
                    'rendering',
                    80 + (15 * cluster_id // len(tts_paths)),
                    f'Creating video for topic {cluster_id + 1}...'
                )
                video_out = make_summary_video(
                    keyframes_dir=keyframes_dir,
                    tts_audio_relpath=tts_path,
                    processed_dir=CONFIG['PROCESSED_DIR'],
                    video_id=video_id,
                    cluster_id=cluster_id,
                    subtitles=summaries[cluster_id],
                )
                summary_videos.append(video_out)
            
            # 7. Finalize with enhanced metadata
            result = {
                'video_id': video_id,
                'summaries': summaries,
                'keywords': cluster_keywords,
                'summary_videos': summary_videos,
                'source_language': detected_language,
                'target_language': target_language,
                'ocr_enabled': enable_ocr,
                'ocr_summary': ocr_summary if enable_ocr else {},
                'total_topics': len(clustered),
                'processing_stats': {
                    'segments_count': len(segments),
                    'keyframes_count': len(os.listdir(keyframes_dir)) if keyframes_dir and os.path.exists(keyframes_dir) else 0,
                    'translation_used': detected_language != target_language
                },
                'status': 'completed',
                'progress': 100,
                'message': 'Processing complete!'
            }
            
            # Update status with result
            processing_status[video_id].update(result)
            
        except Exception as e:
            logger.error(f"Error processing video: {str(e)}\n{traceback.format_exc()}")
            update_processing_status(
                video_id,
                'error',
                0,
                'An error occurred during processing',
                str(e)
            )
    
    # Start processing in background
    from threading import Thread
    thread = Thread(target=process_video)
    thread.daemon = True
    thread.start()
    
    # Return initial response with video ID
    return jsonify({
        'video_id': video_id,
        'status': 'processing',
        'progress': 0,
        'message': 'Processing started'
    })

@app.route('/api/status/<video_id>')
def get_status(video_id: str):
    """Get the current status of a processing job."""
    status = get_processing_status(video_id)
    return jsonify(status)

@app.route('/api/summary/<video_id>')
def get_summary(video_id: str):
    """Get the summary data for a processed video."""
    summary_path = os.path.join(CONFIG['PROCESSED_DIR'], f"{video_id}_summary.json")
    if os.path.exists(summary_path):
        with open(summary_path, 'r', encoding='utf-8') as f:
            return jsonify(json.load(f))
    return jsonify({'error': 'Summary not found'}), 404

@app.route('/api/stream/<video_id>/<int:cluster_id>')
def stream_video(video_id: str, cluster_id: int):
    """Stream a summary video."""
    video_path = os.path.join(CONFIG['PROCESSED_DIR'], f"{video_id}_summary_{cluster_id}.mp4")
    
    if not os.path.exists(video_path):
        return jsonify({'error': 'Video not found'}), 404
    
    range_header = request.headers.get('Range', None)
    if not range_header:
        return send_from_directory(
            CONFIG['PROCESSED_DIR'],
            f"{video_id}_summary_{cluster_id}.mp4",
            as_attachment=False,
            mimetype='video/mp4'
        )
    start,end=0,0
    # Handle byte range requests for streaming
    def generate():
        nonlocal start,end
        with open(video_path, 'rb') as f:
            f.seek(0, 2)
            file_size = f.tell()
            start = 0
            end = file_size - 1
            
            # Parse range header
            range_header = request.headers.get('Range')
            if range_header:
                range_match = re.search(r'bytes=(\d*)-(\d*)', range_header)
                if range_match.group(1):
                    start = int(range_match.group(1))
                if range_match.group(2):
                    end = int(range_match.group(2))
            
            chunk_size = 1024 * 1024  # 1MB chunks
            f.seek(start)
            
            while start <= end:
                chunk = f.read(min(chunk_size, end - start + 1))
                if not chunk:
                    break
                yield chunk
                start += len(chunk)
    
    # Send response with appropriate headers
    file_size = os.path.getsize(video_path)
    response = Response(
        stream_with_context(generate()),
        206,  # Partial Content
        mimetype='video/mp4',
        direct_passthrough=True
    )
    
    response.headers.add('Content-Range', f'bytes {start}-{end}/{file_size}')
    response.headers.add('Accept-Ranges', 'bytes')
    response.headers.add('Content-Length', str(end - start + 1))
    
    return response

@app.route('/processed/<path:path>')
def send_processed_file(path):
    """Serve processed files with proper caching headers."""
    response = send_from_directory(CONFIG['PROCESSED_DIR'], path)
    # Cache for 1 day
    response.headers['Cache-Control'] = 'public, max-age=86400'
    return response

@app.route('/srt/<video_id>')
def get_srt(video_id: str):
    """Get SRT subtitles for a video."""
    srt_path = os.path.join(CONFIG['PROCESSED_DIR'], f"{video_id}.srt")
    if os.path.exists(srt_path):
        response = send_from_directory(CONFIG['PROCESSED_DIR'], f"{video_id}.srt")
        response.headers['Content-Type'] = 'text/plain; charset=utf-8'
        return response
    return jsonify({'error': 'Subtitles not found'}), 404

@app.route('/keyframes/<video_id>')
def get_keyframes(video_id: str):
    """Get list of keyframes for a video."""
    keyframes_dir = os.path.join(CONFIG['PROCESSED_DIR'], f"{video_id}_keyframes")
    if os.path.exists(keyframes_dir):
        try:
            files = sorted([f for f in os.listdir(keyframes_dir) 
                          if f.lower().endswith(('.jpg', '.jpeg', '.png'))])
            return jsonify({
                'keyframes': [f"/processed/{video_id}_keyframes/{f}" for f in files]
            })
        except Exception as e:
            logger.error(f"Error listing keyframes: {e}")
            return jsonify({'error': 'Error listing keyframes'}), 500
    return jsonify({'keyframes': []})

def cleanup_old_files():
    """Clean up old temporary files."""
    try:
        now = time.time()
        max_age = 24 * 3600  # 1 day in seconds
        
        for dir_path in [CONFIG['UPLOAD_DIR'], CONFIG['PROCESSED_DIR']]:
            if not os.path.exists(dir_path):
                continue
                
            for filename in os.listdir(dir_path):
                file_path = os.path.join(dir_path, filename)
                try:
                    # Delete files older than max_age
                    if os.path.isfile(file_path):
                        file_age = now - os.path.getmtime(file_path)
                        if file_age > max_age:
                            os.remove(file_path)
                            logger.info(f"Deleted old file: {file_path}")
                    # Clean up old temporary directories
                    elif os.path.isdir(file_path) and file_path.endswith('_temp'):
                        import shutil
                        shutil.rmtree(file_path, ignore_errors=True)
                        logger.info(f"Deleted temp directory: {file_path}")
                except Exception as e:
                    logger.error(f"Error cleaning up {file_path}: {e}")
    except Exception as e:
        logger.error(f"Error in cleanup_old_files: {e}")

if __name__ == '__main__':
    # Ensure directories exist
    for dir_path in [CONFIG['UPLOAD_DIR'], CONFIG['PROCESSED_DIR']]:
        os.makedirs(dir_path, exist_ok=True)
    
    # Set up cleanup job
    from apscheduler.schedulers.background import BackgroundScheduler
    scheduler = BackgroundScheduler()
    scheduler.add_job(func=cleanup_old_files, trigger='interval', hours=1)
    scheduler.start()
    
    # Start the Flask app
    try:
        app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 5001)), debug=False, use_reloader=False)
    except (KeyboardInterrupt, SystemExit):
        scheduler.shutdown()
</file>

<file path="README.md">
# üé¨ AI-Powered Video Summarizer

An intelligent video summarization system that automatically processes long videos and creates concise, meaningful summaries with AI-generated voiceovers and visual highlights. Built with comprehensive support for all major Indian languages and international languages.

## üåü Key Features

### üéØ Core Functionality
- **Multi-Source Input**: Supports YouTube URLs and direct video uploads
- **Intelligent Processing**: Extracts subtitles, audio, keyframes, and performs topic clustering
- **AI Summarization**: Creates topic-based summaries with natural language processing
- **Video Generation**: Produces summarized videos with keyframes and AI voiceovers
- **Interactive Topics**: Users can select specific topics for detailed explanation

### üåç Language Support
- **22 Official Indian Languages**: Hindi, Bengali, Telugu, Marathi, Tamil, Gujarati, Urdu, Kannada, Malayalam, Punjabi, Odia, Assamese, Nepali, Sanskrit, and more
- **International Languages**: English, Arabic, Chinese, Spanish, French, German, Japanese, Korean, Russian, Portuguese, and others
- **Language Translation**: Automatic translation between any supported languages
- **Native Voice Support**: High-quality Text-to-Speech in all supported languages

### üîç Advanced Features
- **OCR Analysis**: Extracts text from video frames for enhanced context
- **Topic Clustering**: Uses advanced ML algorithms (LDA, NMF, K-means) for content organization
- **Keyframe Intelligence**: Smart keyframe extraction with similarity detection
- **Multiple Resolutions**: Support for 480p, 720p, and 1080p output
- **Progress Tracking**: Real-time processing status updates

## üöÄ Quick Start

### Prerequisites
- Python 3.11
- FFmpeg installed on your system
- At least 4GB RAM (8GB recommended for large videos)
- Internet connection for translation services

### Installation

1. **Clone the Repository**
   ```bash
   git clone https://github.com/meghanavemala/major-project
   cd video-summarizer
   ```

2. **Create Virtual Environment**
   ```bash
   python -m venv venv
   
   # On Windows
   venv\Scripts\activate
   
   # On macOS/Linux
   source venv/bin/activate
   ```

3. **Install Dependencies**
   ```bash
   pip install -r requirements.txt
   ```

4. **Install System Dependencies**

   **Windows:**
   - Install FFmpeg from https://ffmpeg.org/download.html
   - Add FFmpeg to system PATH
   - Install Tesseract OCR from https://github.com/UB-Mannheim/tesseract/wiki

   **macOS:**
   ```bash
   brew install ffmpeg tesseract
   ```

   **Ubuntu/Debian:**
   ```bash
   sudo apt update
   sudo apt install ffmpeg tesseract-ocr tesseract-ocr-hin tesseract-ocr-ben tesseract-ocr-tam tesseract-ocr-tel tesseract-ocr-kan tesseract-ocr-mal tesseract-ocr-urd tesseract-ocr-guj tesseract-ocr-pan tesseract-ocr-ori tesseract-ocr-asm tesseract-ocr-mar tesseract-ocr-nep tesseract-ocr-san
   ```

5. **Download NLTK Data**
   ```bash
   python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('punkt_tab')"
   ```

6. **Create Required Directories**
   ```bash
   mkdir uploads processed static templates
   ```

### Running the Application

1. **Start the Flask Server**
   ```bash
   python app.py
   ```

2. **Access the Application**
   Open your browser and navigate to: `http://localhost:5000`

## üìñ How It Works

### Step-by-Step Process

1. **Video Input**: User uploads a video file or provides a YouTube URL
2. **Language Selection**: User selects source language and target language for summary
3. **Processing Pipeline**:
   - **Audio Extraction**: Extracts audio from video using FFmpeg
   - **Transcription**: Uses OpenAI Whisper for speech-to-text conversion
   - **Keyframe Extraction**: Identifies important visual moments
   - **OCR Analysis**: Extracts text from keyframes for additional context
   - **Topic Clustering**: Groups content into coherent topics using ML
   - **Summarization**: Generates concise summaries for each topic
   - **Translation**: Converts summaries to target language if needed
   - **Voice Generation**: Creates AI voiceovers using gTTS
   - **Video Assembly**: Combines keyframes, audio, and subtitles

4. **Output**: Interactive interface showing topic-based summaries with playable videos

### Architecture Overview

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Frontend      ‚îÇ    ‚îÇ   Flask API     ‚îÇ    ‚îÇ   Processing    ‚îÇ
‚îÇ   (HTML/JS)     ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   (app.py)      ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   Pipeline      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                ‚îÇ
                                ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ         Utils Modules           ‚îÇ
                    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                    ‚îÇ ‚Ä¢ transcriber.py (Whisper)      ‚îÇ
                    ‚îÇ ‚Ä¢ keyframes.py (CV + OCR)       ‚îÇ
                    ‚îÇ ‚Ä¢ clustering.py (ML/NLP)        ‚îÇ
                    ‚îÇ ‚Ä¢ summarizer.py (Transformers)  ‚îÇ
                    ‚îÇ ‚Ä¢ translator.py (Multi-lang)    ‚îÇ
                    ‚îÇ ‚Ä¢ tts.py (Voice Generation)     ‚îÇ
                    ‚îÇ ‚Ä¢ video_maker.py (Assembly)     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üõ†Ô∏è Configuration

### Environment Variables
Create a `.env` file in the project root:

```env
# Flask Configuration
FLASK_SECRET_KEY=your-secret-key-here
FLASK_ENV=production

# Processing Limits
MAX_CONTENT_LENGTH=104857600  # 100MB
MAX_VIDEO_DURATION=3600       # 60 minutes

# API Keys (Optional)
GOOGLE_TRANSLATE_API_KEY=your-google-api-key
AZURE_TRANSLATE_KEY=your-azure-key

# OCR Configuration
TESSERACT_CMD=/usr/bin/tesseract  # Path to tesseract binary
ENABLE_OCR=true

# Model Configuration
WHISPER_MODEL_SIZE=medium
SUMMARIZATION_MODEL=facebook/bart-large-cnn
TRANSLATION_METHOD=google  # google, m2m100, auto
```

### Processing Settings

You can customize processing parameters in each utility module:

- **Transcription**: Model size, language detection threshold
- **Keyframes**: Frame interval, similarity threshold, resolution
- **Clustering**: Number of topics, clustering method, minimum topic size
- **Summarization**: Summary length, model selection
- **Translation**: Translation service, fallback methods
- **TTS**: Voice selection, speech rate, audio quality

## üìö API Reference

### Main Endpoints

#### `POST /api/process`
Initiates video processing.

**Parameters:**
- `video` (file) or `video_url` (string): Video input
- `source_language` (string): Source language code
- `target_language` (string): Target language for summary
- `voice` (string): Voice ID for TTS
- `resolution` (string): Output resolution (480p, 720p, 1080p)
- `summary_length` (string): short, medium, long

**Response:**
```json
{
  "video_id": "unique-video-id",
  "status": "processing",
  "progress": 0,
  "message": "Processing started"
}
```

#### `GET /api/status/<video_id>`
Get processing status.

**Response:**
```json
{
  "status": "completed",
  "progress": 100,
  "message": "Processing complete!",
  "summaries": [...],
  "keywords": [...],
  "summary_videos": [...]
}
```

#### `GET /api/stream/<video_id>/<cluster_id>`
Stream summary video for a specific topic.

#### `GET /api/summary/<video_id>`
Get complete summary data including transcripts and metadata.

### Supported Languages

#### Indian Languages (22 Official Languages)
| Language | Code | Script | TTS Support | OCR Support |
|----------|------|--------|-------------|-------------|
| Hindi | `hi` | ‡§¶‡•á‡§µ‡§®‡§æ‡§ó‡§∞‡•Ä | ‚úÖ | ‚úÖ |
| Bengali | `bn` | ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ | ‚úÖ | ‚úÖ |
| Telugu | `te` | ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å | ‚úÖ | ‚úÖ |
| Marathi | `mr` | ‡§Æ‡§∞‡§æ‡§†‡•Ä | ‚úÖ | ‚úÖ |
| Tamil | `ta` | ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç | ‚úÖ | ‚úÖ |
| Gujarati | `gu` | ‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä | ‚úÖ | ‚úÖ |
| Urdu | `ur` | ÿßÿ±ÿØŸà | ‚úÖ | ‚úÖ |
| Kannada | `kn` | ‡≤ï‡≤®‡≥ç‡≤®‡≤° | ‚úÖ | ‚úÖ |
| Malayalam | `ml` | ‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç | ‚úÖ | ‚úÖ |
| Punjabi | `pa` | ‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä | ‚úÖ | ‚úÖ |
| Odia | `or` | ‡¨ì‡¨°‡¨º‡¨ø‡¨Ü | ‚úÖ | ‚úÖ |
| Assamese | `as` | ‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ | ‚úÖ | ‚úÖ |
| Nepali | `ne` | ‡§®‡•á‡§™‡§æ‡§≤‡•Ä | ‚úÖ | ‚úÖ |
| Sanskrit | `sa` | ‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§ | ‚úÖ | ‚úÖ |

#### International Languages
English, Arabic, Chinese, Spanish, French, German, Japanese, Korean, Russian, Portuguese, Italian, Dutch, Turkish, Polish, Thai, Vietnamese, Indonesian, Malay

## üîß Advanced Usage

### Custom Model Integration

You can integrate custom models for specialized use cases:

```python
# Example: Custom summarization model
from utils.summarizer import load_summarizer

# Load custom model
custom_model = load_summarizer('your-custom-model-name')

# Use in processing pipeline
summary = custom_model.summarize(text, language='hindi')
```

### Batch Processing

For processing multiple videos:

```python
from utils import process_video_batch

videos = [
    {'path': 'video1.mp4', 'language': 'hindi'},
    {'path': 'video2.mp4', 'language': 'tamil'},
]

results = process_video_batch(videos, target_language='english')
```

### Custom Translation

Add support for additional translation services:

```python
from utils.translator import register_translation_method

def custom_translate(text, source, target):
    # Your custom translation logic
    return translated_text

register_translation_method('custom', custom_translate)
```

## üß™ Testing

Run the test suite:

```bash
# Install test dependencies
pip install pytest pytest-cov

# Run all tests
pytest

# Run with coverage
pytest --cov=utils tests/

# Run specific test category
pytest tests/test_transcription.py -v
```

## üêõ Troubleshooting

### Common Issues

1. **FFmpeg not found**
   - Ensure FFmpeg is installed and in system PATH
   - Test with: `ffmpeg -version`

2. **Tesseract OCR errors**
   - Install language packs for your target languages
   - Verify installation: `tesseract --list-langs`

3. **Memory issues with large videos**
   - Reduce video resolution before processing
   - Use smaller Whisper models
   - Increase system swap space

4. **Translation API limits**
   - Switch to offline translation models
   - Implement API key rotation
   - Use rate limiting

5. **Slow processing**
   - Use GPU acceleration if available
   - Reduce frame extraction interval
   - Use smaller AI models

### Performance Optimization

- **GPU Support**: Install CUDA for faster processing
- **Model Caching**: Models are cached after first load
- **Parallel Processing**: Multiple videos can be processed simultaneously
- **Resource Management**: Automatic cleanup of temporary files

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup

1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Install development dependencies: `pip install -r requirements-dev.txt`
4. Make your changes and add tests
5. Run tests: `pytest`
6. Submit a pull request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üôè Acknowledgments

- **OpenAI Whisper** for state-of-the-art speech recognition
- **Hugging Face Transformers** for NLP models
- **Google Translate** for translation services
- **gTTS** for text-to-speech synthesis
- **OpenCV** for computer vision
- **Flask** for the web framework
- **The open-source community** for various libraries and tools

## üìä Performance Metrics

| Video Length | Processing Time | Memory Usage | Accuracy |
|--------------|----------------|--------------|----------|
| 5 minutes    | ~2 minutes     | 2GB         | 95%      |
| 15 minutes   | ~6 minutes     | 4GB         | 94%      |
| 30 minutes   | ~12 minutes    | 6GB         | 93%      |
| 60 minutes   | ~25 minutes    | 8GB         | 92%      |

*Performance metrics are approximate and may vary based on hardware and content complexity.*

## üîÆ Future Enhancements

- [ ] Real-time video processing
- [ ] Multi-speaker identification
- [ ] Advanced emotion detection
- [ ] Custom model training interface
- [ ] Mobile app support
- [ ] Cloud deployment options
- [ ] Advanced analytics dashboard
- [ ] Video editing capabilities

## üìû Support

- **Documentation**: [Wiki](https://github.com/yourusername/video-summarizer/wiki)
- **Issues**: [GitHub Issues](https://github.com/yourusername/video-summarizer/issues)
- **Discussions**: [GitHub Discussions](https://github.com/yourusername/video-summarizer/discussions)
- **Email**: support@video-summarizer.com

---

Built with ‚ù§Ô∏è for the global community to make video content more accessible across languages and cultures.
</file>

<file path="requirements.txt">
# ------------------------
# Core Web Framework
# ------------------------
Flask==2.3.3
Werkzeug>=2.3.0

# ------------------------
# AI / NLP / ML
# ------------------------
transformers==4.31.0
torch==2.0.1
torchaudio>=2.0.2
openai-whisper @ git+https://github.com/openai/whisper.git
scikit-learn==1.3.0
numpy==1.25.0
pandas==2.0.3

# ------------------------
# Audio / Speech Processing
# ------------------------
pydub==0.25.1
librosa==0.10.0
soundfile==0.12.1

# ------------------------
# Image / Video Processing
# ------------------------
opencv-python==4.8.0.76
Pillow==9.5.0
pytesseract==0.3.10

# ------------------------
# Clustering & Summarization
# ------------------------
nltk==3.8.1
sentence-transformers==2.2.2

# ------------------------
# Translation
# ------------------------
deep-translator==1.11.4

# ------------------------
# Utilities
# ------------------------
tqdm==4.66.1
python-dotenv==1.0.0
requests==2.31.0

# ------------------------
# Video Generation / Editing
# ------------------------
moviepy==1.0.3
ffmpeg-python==0.2.0
</file>

<file path="SETUP.md">
# üöÄ Quick Setup Guide for AI Video Summarizer

This guide will help you set up the AI Video Summarizer quickly and efficiently.

## üìã Prerequisites Checklist

Before starting, ensure you have:

- [ ] Python 3.8 or higher
- [ ] At least 4GB RAM (8GB recommended)
- [ ] 10GB free disk space
- [ ] Internet connection for translation services
- [ ] Git installed

## üîß System Dependencies

### Windows
```bash
# Install FFmpeg
# Download from https://ffmpeg.org/download.html
# Add to system PATH

# Install Tesseract OCR
# Download from https://github.com/UB-Mannheim/tesseract/wiki
# Install with all language packs
```

### macOS
```bash
# Install Homebrew if not already installed
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Install dependencies
brew install ffmpeg tesseract tesseract-lang
```

### Ubuntu/Debian
```bash
# Update package list
sudo apt update

# Install FFmpeg
sudo apt install ffmpeg

# Install Tesseract with Indian language support
sudo apt install tesseract-ocr tesseract-ocr-hin tesseract-ocr-ben tesseract-ocr-tam tesseract-ocr-tel tesseract-ocr-kan tesseract-ocr-mal tesseract-ocr-urd tesseract-ocr-guj tesseract-ocr-pan tesseract-ocr-ori tesseract-ocr-asm tesseract-ocr-mar tesseract-ocr-nep tesseract-ocr-san

# Install additional dependencies
sudo apt install python3-dev python3-pip python3-venv build-essential
```

## üì• Installation Steps

### 1. Clone and Setup
```bash
# Clone the repository
git clone https://github.com/meghanavemala/major-project.git
cd major-project

# Create virtual environment
python -m venv venv

# Activate virtual environment
# Windows:
venv\Scripts\activate
# macOS/Linux:
source venv/bin/activate

# Upgrade pip
pip install --upgrade pip
```

### 2. Install Python Dependencies
```bash
# Install all required packages
pip install -r requirements.txt

# Download NLTK data
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('punkt_tab')"

# Test installation
python -c "import torch; import whisper; import cv2; print('‚úÖ All dependencies installed successfully!')"
```

### 3. Verify System Tools
```bash
# Test FFmpeg
ffmpeg -version

# Test Tesseract
tesseract --version
tesseract --list-langs

# Test Python imports
python -c "
import whisper
import transformers
import pytesseract
import easyocr
print('‚úÖ All AI models accessible!')
"
```

### 4. Create Configuration
```bash
# Create environment file
cat > .env << EOF
# Flask Configuration
FLASK_SECRET_KEY=your-secret-key-change-this
FLASK_ENV=production

# Processing Limits
MAX_CONTENT_LENGTH=104857600
MAX_VIDEO_DURATION=3600

# OCR Configuration
ENABLE_OCR=true

# Model Settings
WHISPER_MODEL_SIZE=medium
TRANSLATION_METHOD=google
EOF
```

### 5. Test Run
```bash
# Start the application
python app.py

# Check if it's running
curl http://localhost:5000
```

## üåê Browser Setup

1. Open your browser
2. Navigate to `http://localhost:5000`
3. You should see the AI Video Summarizer interface

## üîç Troubleshooting

### Common Issues and Solutions

#### ‚ùå "FFmpeg not found"
```bash
# Windows: Ensure FFmpeg is in PATH
where ffmpeg

# macOS/Linux: Check installation
which ffmpeg

# If not found, reinstall following system dependencies above
```

#### ‚ùå "No module named 'torch'"
```bash
# Reinstall PyTorch
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
```
```bash
# If want to use GPU
pip uninstall torch torchvision torchaudio
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

#### ‚ùå "Tesseract not found"
```bash
# Check Tesseract installation
tesseract --version

# If not found, install following system dependencies
# Then test Python integration:
python -c "import pytesseract; print(pytesseract.image_to_string('test.png'))"
```

#### ‚ùå "CUDA out of memory"
```bash
# Use CPU-only mode by setting in your environment:
export CUDA_VISIBLE_DEVICES=""
# Or modify model sizes in config to use smaller models
```

#### ‚ùå Translation errors
```bash
# Check internet connection
ping translate.googleapis.com

# Fallback to offline translation
# Edit app.py and set: TRANSLATION_METHOD=m2m100
```

#### ‚ùå Port already in use
```bash
# Check what's using port 5000
# Windows:
netstat -ano | findstr :5000
# macOS/Linux:
lsof -i :5000

# Kill the process or use different port:
python app.py --port 5001
```

## üß™ Test With Sample Video

1. Download a short test video:
```bash
# Create test directory
mkdir test_videos
cd test_videos

# Download a sample video (or use your own)
# Upload it through the web interface
```

2. Test basic functionality:
   - Upload the video
   - Select source language (or auto-detect)
   - Select target language
   - Choose voice and quality settings
   - Click "Process Video"

3. Expected results:
   - Processing progress should show
   - Topics should be generated
   - Summary videos should be playable
   - Downloads should work

## ‚ö° Performance Optimization

### For Better Speed:
```bash
# Use GPU if available
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Reduce model sizes for faster processing
# Edit the configuration to use smaller models:
# WHISPER_MODEL_SIZE=base
# Use 480p resolution instead of 1080p
```

### For Better Quality:
```bash
# Use larger models (slower but more accurate)
# WHISPER_MODEL_SIZE=large
# Enable all OCR features
# Use 1080p resolution
```

## üîí Production Deployment

### Security Setup:
```bash
# Generate secure secret key
python -c "import secrets; print(secrets.token_hex(32))"

# Update .env with the generated key
# Set FLASK_ENV=production
```

### Using Gunicorn:
```bash
# Install gunicorn
pip install gunicorn

# Run with gunicorn
gunicorn --workers 4 --bind 0.0.0.0:8000 app:app
```

### Using Docker:
```bash
# Build Docker image
docker build -t video-summarizer .

# Run container
docker run -p 5000:5000 -v $(pwd)/uploads:/app/uploads -v $(pwd)/processed:/app/processed video-summarizer
```

## üìä Monitoring and Logs

### Check Application Logs:
```bash
# View recent logs
tail -f app.log

# Check for errors
grep ERROR app.log
```

### Monitor Performance:
```bash
# Check disk usage
df -h

# Check memory usage
free -h

# Monitor processing
ps aux | grep python
```

## üÜò Getting Help

If you encounter issues:

1. **Check the logs**: Look at `app.log` for error messages
2. **Verify dependencies**: Ensure all system tools are installed
3. **Test components**: Run individual tests for each component
4. **Check GitHub Issues**: Look for similar problems
5. **Create an Issue**: Provide logs and system information

### System Information Template:
```bash
# Get system info for bug reports
echo "OS: $(uname -a)"
echo "Python: $(python --version)"
echo "FFmpeg: $(ffmpeg -version | head -1)"
echo "Tesseract: $(tesseract --version | head -1)"
echo "GPU: $(nvidia-smi | head -1 || echo 'No NVIDIA GPU')"
pip list | grep -E "(torch|whisper|opencv|transformers)"
```

## üéâ You're Ready!

Congratulations! Your AI Video Summarizer is now set up and ready to use. Start by uploading a short video to test all features.

### Next Steps:
- Try different languages
- Experiment with voice options
- Test topic selection
- Explore download features
- Customize settings for your needs

Happy summarizing! üé¨‚ú®
</file>

<file path="utils/downloader.py">
import os
import re
import uuid
from urllib.parse import urlparse
import yt_dlp
import subprocess
def is_youtube_url(url: str) -> bool:
    """
    Check if the given string is a valid YouTube URL.
    
    Args:
        url (str): The URL to check
        
    Returns:
        bool: True if the URL is a valid YouTube URL, False otherwise
    """
    youtube_regex = (
        r'(https?://)?(www\.)?'
        '(youtube|youtu|youtube-nocookie)\.(com|be)/'
        '(watch\?v=|embed/|v/|.+/)?([\w-]{11})(?:\?[\w-]*=[\w-]*(?:&[\w-]*=[\w-]*)*)?$'
    )
    youtube_regex_match = re.match(youtube_regex, url)
    return youtube_regex_match is not None

def handle_video_upload_or_download(request, upload_dir):
    video_id = str(uuid.uuid4())
    if 'video' in request.files and request.files['video']:
        f = request.files['video']
        path = os.path.join(upload_dir, f"{video_id}_{f.filename}")
        file_content = f.read()
        
        # Save to disk manually from content
        with open(path, 'wb') as out_file:
            out_file.write(file_content)
        return path, video_id
    elif 'yt_url' in request.form and request.form['yt_url']:
        # yt = YouTube(request.form['yt_url'])
        # stream = yt.streams.filter(file_extension='mp4', progressive=True).first()
        # path = stream.download(output_path=upload_dir, filename=f"{video_id}.mp4")
        # return path, video_id
            temp_file = "temp_video.mp4"
            ydl_opts = {
                'outtmpl': temp_file,
                'format': 'bestvideo[height<=720]+bestaudio/best[height<=720]',
                'merge_output_format': 'mp4'
            }

            with yt_dlp.YoutubeDL(ydl_opts) as ydl:
                ydl.download(['yt_url'])

            # Compress with ffmpeg for OCR-friendly video
            compressed_path = upload_dir+f"{video_id}.mp4"
            subprocess.run([
                "ffmpeg", "-i", temp_file,
                "-vcodec", "libx264", "-crf", "28",  # Higher CRF = more compression
                "-preset", "fast",
                "-acodec", "aac", "-b:a", "96k",
                compressed_path
            ])

            os.remove(temp_file)
            return compressed_path,video_id
    else:
        raise ValueError("No upload or url provided")
</file>

<file path="utils/summarizer.py">
import logging
from typing import List, Dict, Any, Optional
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import string

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Download required NLTK data
# try:
#     nltk.data.find('tokenizers/punkt')
#     nltk.data.find('corpora/stopwords')
# except LookupError:
#     logger.info("Downloading NLTK data...")
#     nltk.download('punkt')
#     nltk.download('stopwords')

# Initialize models as None. They will be loaded on first use.
SUMMARIZERS = {
    'en': None,  # English
    'hi': None,  # Hindi
    'kn': None   # Kannada
}

# Model configurations for different languages
MODEL_CONFIGS = {
    'en': {
        'model_name': 'facebook/bart-large-cnn',
        'min_length': 30,
        'max_length': 130,
        'repetition_penalty': 2.5,
        'length_penalty': 1.0,
        'num_beams': 4,
    },
    'hi': {
        'model_name': 'csebuetnlp/mT5_multilingual_XLSum',
        'min_length': 30,
        'max_length': 100,
        'repetition_penalty': 2.5,
        'length_penalty': 1.0,
        'num_beams': 4,
    },
    'kn': {
        'model_name': 'csebuetnlp/mT5_multilingual_XLSum',
        'min_length': 30,
        'max_length': 100,
        'repetition_penalty': 2.5,
        'length_penalty': 1.0,
        'num_beams': 4,
    }
}

def preprocess_text(text: str, language: str = 'en') -> str:
    """Preprocess text before summarization."""
    # Remove extra whitespace
    text = ' '.join(text.split())
    
    # Remove special characters and numbers
    if language == 'en':
        text = ''.join([char for char in text if char.isalnum() or char.isspace() or char in ',.!?'])
    
    return text

def extract_key_sentences(text: str, num_sentences: int = 3, language: str = 'en') -> List[str]:
    """Extract key sentences using a simple heuristic."""
    try:
        # Tokenize into sentences
        sentences = sent_tokenize(text, language='english' if language == 'en' else language)
        
        # Simple scoring based on word frequency
        words = [word.lower() for word in word_tokenize(text) 
                if word.lower() not in stopwords.words('english' if language == 'en' else language) 
                and word not in string.punctuation]
        
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        # Score sentences based on word frequency
        sentence_scores = {}
        for i, sentence in enumerate(sentences):
            for word in word_tokenize(sentence.lower()):
                if word in word_freq:
                    sentence_scores[i] = sentence_scores.get(i, 0) + word_freq[word]
        
        # Get top N sentences
        top_sentences = sorted(sentence_scores.items(), key=lambda x: x[1], reverse=True)[:num_sentences]
        top_sentences = sorted([i[0] for i in top_sentences])
        
        return [sentences[i] for i in top_sentences]
    except Exception as e:
        logger.warning(f"Error in extract_key_sentences: {e}")
        return text[:500].split('. ')[:3]  # Fallback: first few sentences

def load_summarizer(language: str = 'en') -> Any:
    """Load the appropriate summarization model for the specified language."""
    global SUMMARIZERS
    
    if language not in SUMMARIZERS:
        logger.warning(f"Unsupported language: {language}. Defaulting to English.")
        language = 'en'
    
    if SUMMARIZERS[language] is None:
        try:
            model_name = MODEL_CONFIGS[language]['model_name']
            logger.info(f"Loading {language} summarization model: {model_name}")
            
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            logger.info(f"Using device: {device}")
            
            # Load tokenizer and model
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
            
            # Create pipeline
            SUMMARIZERS[language] = {
                'pipeline': pipeline(
                    'summarization',
                    model=model,
                    tokenizer=tokenizer,
                    device=0 if device == 'cuda' else -1,
                    framework='pt'
                ),
                'config': MODEL_CONFIGS[language]
            }
            logger.info(f"{language.capitalize()} summarization model loaded successfully.")
            
        except Exception as e:
            logger.error(f"Error loading {language} summarization model: {e}")
            return None
    
    return SUMMARIZERS[language]

def summarize_cluster(
    cluster: List[Dict[str, Any]], 
    language: str = 'en',
    use_extractive: bool = False
) -> str:
    """
    Generate a summary for a cluster of text segments.
    
    Args:
        cluster: List of text segments with 'text' keys
        language: Language code ('en', 'hi', 'kn')
        use_extractive: Whether to use extractive summarization (faster but less coherent)
        
    Returns:
        Generated summary text
    """
    if not cluster:
        return "No content to summarize."
    
    # Combine all text from the cluster
    cluster_text = " ".join([seg.get('text', '') for seg in cluster])
    
    if not cluster_text.strip():
        return "No content to summarize."
    
    language = language.lower()
    if language not in ['en', 'hi', 'kn']:
        logger.warning(f"Unsupported language: {language}. Defaulting to English.")
        language = 'en'
    
    try:
        # Preprocess text
        cluster_text = preprocess_text(cluster_text, language)
        
        # For very short texts, just return as is
        if len(word_tokenize(cluster_text)) < 30:
            return cluster_text
        
        # Use extractive summarization for non-English or as a fallback
        if use_extractive or language in ['hi', 'kn']:
            key_sentences = extract_key_sentences(cluster_text, num_sentences=3, language=language)
            return ' '.join(key_sentences)
        
        # Use abstractive summarization for English
        summarizer = load_summarizer(language)
        if not summarizer:
            raise Exception(f"Failed to load {language} summarization model")
        
        # Get model config
        config = summarizer['config']
        
        # Split long text into chunks if needed (to avoid token limits)
        max_chunk_length = 1024 if language == 'en' else 768
        if len(cluster_text) > max_chunk_length:
            # Simple chunking by sentences
            sentences = sent_tokenize(cluster_text)
            chunks = []
            current_chunk = []
            current_length = 0
            
            for sent in sentences:
                sent_length = len(word_tokenize(sent))
                if current_length + sent_length > max_chunk_length and current_chunk:
                    chunks.append(' '.join(current_chunk))
                    current_chunk = []
                    current_length = 0
                current_chunk.append(sent)
                current_length += sent_length
            
            if current_chunk:
                chunks.append(' '.join(current_chunk))
            
            # Summarize each chunk
            chunk_summaries = []
            for chunk in chunks:
                summary = summarizer['pipeline'](
                    chunk,
                    min_length=config['min_length'],
                    max_length=config['max_length'],
                    repetition_penalty=config['repetition_penalty'],
                    length_penalty=config['length_penalty'],
                    num_beams=config['num_beams'],
                    truncation=True
                )
                chunk_summaries.append(summary[0]['summary_text'])
            
            # Combine chunk summaries
            combined_summary = ' '.join(chunk_summaries)
            
            # Final summary of combined chunk summaries if still too long
            if len(word_tokenize(combined_summary)) > 100:
                final_summary = summarizer['pipeline'](
                    combined_summary,
                    min_length=config['min_length'],
                    max_length=config['max_length'],
                    repetition_penalty=config['repetition_penalty'],
                    length_penalty=config['length_penalty'],
                    num_beams=config['num_beams'],
                    truncation=True
                )
                return final_summary[0]['summary_text']
            return combined_summary
        
        else:
            # Process in one go if text is short enough
            summary = summarizer['pipeline'](
                cluster_text,
                min_length=config['min_length'],
                max_length=config['max_length'],
                repetition_penalty=config['repetition_penalty'],
                length_penalty=config['length_penalty'],
                num_beams=config['num_beams'],
                truncation=True
            )
            return summary[0]['summary_text']
    
    except Exception as e:
        logger.error(f"Error in summarize_cluster: {e}", exc_info=True)
        # Fallback to extractive summarization
        try:
            key_sentences = extract_key_sentences(cluster_text, num_sentences=3, language=language)
            return ' '.join(key_sentences)
        except Exception as e2:
            logger.error(f"Fallback summarization also failed: {e2}")
            return cluster_text[:500] + "..."  # Return first 500 chars as fallback
</file>

<file path="utils/translator.py">
"""
Language Translation Utility Module

This module provides comprehensive translation capabilities for the video summarizer,
enabling conversion between different Indian languages and international languages.
It supports both Google Translate API and offline translation models.

Author: Video Summarizer Team
Created: 2024
"""

import os
import logging
import json
from typing import Dict, List, Optional, Tuple, Union
# Optional: Google Translate client (used when available)
try:
    from googletrans import Translator, LANGUAGES  # type: ignore
    GOOGLETRANS_AVAILABLE = True
except Exception:
    # If googletrans is not installed or fails to import, we gracefully degrade
    GOOGLETRANS_AVAILABLE = False
    Translator = None  # type: ignore
    LANGUAGES = {}  # type: ignore
import torch
from transformers import (
    MarianMTModel, MarianTokenizer, 
    M2M100ForConditionalGeneration, M2M100Tokenizer,
    AutoTokenizer, AutoModelForSeq2SeqLM
)
import time
import pickle
from pathlib import Path

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Language code mappings - comprehensive support for Indian languages
LANGUAGE_MAPPINGS = {
    # Indian Languages (official)
    'hindi': {'code': 'hi', 'google': 'hi', 'iso': 'hi', 'native': '‡§π‡§ø‡§®‡•ç‡§¶‡•Ä'},
    'bengali': {'code': 'bn', 'google': 'bn', 'iso': 'bn', 'native': '‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ'},
    'telugu': {'code': 'te', 'google': 'te', 'iso': 'te', 'native': '‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å'},
    'marathi': {'code': 'mr', 'google': 'mr', 'iso': 'mr', 'native': '‡§Æ‡§∞‡§æ‡§†‡•Ä'},
    'tamil': {'code': 'ta', 'google': 'ta', 'iso': 'ta', 'native': '‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç'},
    'gujarati': {'code': 'gu', 'google': 'gu', 'iso': 'gu', 'native': '‡™ó‡´Å‡™ú‡™∞‡™æ‡™§‡´Ä'},
    'urdu': {'code': 'ur', 'google': 'ur', 'iso': 'ur', 'native': 'ÿßÿ±ÿØŸà'},
    'kannada': {'code': 'kn', 'google': 'kn', 'iso': 'kn', 'native': '‡≤ï‡≤®‡≥ç‡≤®‡≤°'},
    'odia': {'code': 'or', 'google': 'or', 'iso': 'or', 'native': '‡¨ì‡¨°‡¨º‡¨ø‡¨Ü'},
    'malayalam': {'code': 'ml', 'google': 'ml', 'iso': 'ml', 'native': '‡¥Æ‡¥≤‡¥Ø‡¥æ‡¥≥‡¥Ç'},
    'punjabi': {'code': 'pa', 'google': 'pa', 'iso': 'pa', 'native': '‡®™‡©∞‡®ú‡®æ‡®¨‡©Ä'},
    'assamese': {'code': 'as', 'google': 'as', 'iso': 'as', 'native': '‡¶Ö‡¶∏‡¶Æ‡ßÄ‡¶Ø‡¶º‡¶æ'},
    'nepali': {'code': 'ne', 'google': 'ne', 'iso': 'ne', 'native': '‡§®‡•á‡§™‡§æ‡§≤‡•Ä'},
    'sanskrit': {'code': 'sa', 'google': 'sa', 'iso': 'sa', 'native': '‡§∏‡§Ç‡§∏‡•ç‡§ï‡•É‡§§'},
    'sindhi': {'code': 'sd', 'google': 'sd', 'iso': 'sd', 'native': 'ÿ≥ŸÜÿØ⁄æ€å'},
    
    # International Languages
    'english': {'code': 'en', 'google': 'en', 'iso': 'en', 'native': 'English'},
    'arabic': {'code': 'ar', 'google': 'ar', 'iso': 'ar', 'native': 'ÿßŸÑÿπÿ±ÿ®Ÿäÿ©'},
    'chinese': {'code': 'zh', 'google': 'zh', 'iso': 'zh', 'native': '‰∏≠Êñá'},
    'spanish': {'code': 'es', 'google': 'es', 'iso': 'es', 'native': 'Espa√±ol'},
    'french': {'code': 'fr', 'google': 'fr', 'iso': 'fr', 'native': 'Fran√ßais'},
    'german': {'code': 'de', 'google': 'de', 'iso': 'de', 'native': 'Deutsch'},
    'japanese': {'code': 'ja', 'google': 'ja', 'iso': 'ja', 'native': 'Êó•Êú¨Ë™û'},
    'korean': {'code': 'ko', 'google': 'ko', 'iso': 'ko', 'native': 'ÌïúÍµ≠Ïñ¥'},
    'russian': {'code': 'ru', 'google': 'ru', 'iso': 'ru', 'native': '–†—É—Å—Å–∫–∏–π'},
    'portuguese': {'code': 'pt', 'google': 'pt', 'iso': 'pt', 'native': 'Portugu√™s'},
}

# Translation model preferences - ordered by quality
TRANSLATION_METHODS = [
    'google_translate',  # Best quality, requires internet
    'm2m100',           # Good multilingual model, offline
    'marian',           # Good for specific language pairs, offline
    'fallback'          # Simple word replacement, last resort
]

# Cache for translation models to avoid reloading
translation_cache = {
    'google_translator': None,
    'm2m100_model': None,
    'm2m100_tokenizer': None,
    'marian_models': {},
    'cache_dir': 'translation_cache'
}

def ensure_cache_dir():
    """Ensure translation cache directory exists."""
    cache_dir = Path(translation_cache['cache_dir'])
    cache_dir.mkdir(exist_ok=True)
    return cache_dir

def get_google_translator():
    """Get or create Google Translator instance."""
    if not GOOGLETRANS_AVAILABLE:
        logger.warning("googletrans not installed; skipping Google Translate. Install with: pip install googletrans==4.0.0rc1")
        return None
    if translation_cache['google_translator'] is None:
        try:
            translation_cache['google_translator'] = Translator()
            logger.info("Google Translator initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Google Translator: {e}")
            return None
    return translation_cache['google_translator']

def get_m2m100_model():
    """Get or load M2M100 multilingual translation model."""
    if translation_cache['m2m100_model'] is None:
        try:
            logger.info("Loading M2M100 multilingual translation model...")
            model_name = "facebook/m2m100_418M"
            
            # Check if CUDA is available
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            logger.info(f"Using device: {device}")
            
            # Load tokenizer and model
            tokenizer = M2M100Tokenizer.from_pretrained(model_name)
            model = M2M100ForConditionalGeneration.from_pretrained(model_name).to(device)
            
            translation_cache['m2m100_tokenizer'] = tokenizer
            translation_cache['m2m100_model'] = model
            
            logger.info("M2M100 model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load M2M100 model: {e}")
            return None, None
    
    return translation_cache['m2m100_model'], translation_cache['m2m100_tokenizer']

def translate_with_google(text: str, source_lang: str, target_lang: str) -> Optional[str]:
    """
    Translate text using Google Translate API.
    
    Args:
        text: Text to translate
        source_lang: Source language code
        target_lang: Target language code
        
    Returns:
        Translated text or None if failed
    """
    if not GOOGLETRANS_AVAILABLE:
        return None
    try:
        translator = get_google_translator()
        if not translator:
            return None
        
        # Convert language codes to Google Translate format
        src_code = LANGUAGE_MAPPINGS.get(source_lang, {}).get('google', source_lang)
        tgt_code = LANGUAGE_MAPPINGS.get(target_lang, {}).get('google', target_lang)
        
        # Perform translation
        result = translator.translate(text, src=src_code, dest=tgt_code)
        translated_text = result.text
        
        logger.info(f"Google Translate: {source_lang} -> {target_lang} successful")
        return translated_text
        
    except Exception as e:
        logger.error(f"Google Translate failed: {e}")
        return None

def translate_with_m2m100(text: str, source_lang: str, target_lang: str) -> Optional[str]:
    """
    Translate text using M2M100 multilingual model.
    
    Args:
        text: Text to translate
        source_lang: Source language code
        target_lang: Target language code
        
    Returns:
        Translated text or None if failed
    """
    try:
        model, tokenizer = get_m2m100_model()
        if not model or not tokenizer:
            return None
        
        # Convert language codes to M2M100 format
        src_code = LANGUAGE_MAPPINGS.get(source_lang, {}).get('code', source_lang)
        tgt_code = LANGUAGE_MAPPINGS.get(target_lang, {}).get('code', target_lang)
        
        # Set source language
        tokenizer.src_lang = src_code
        
        # Tokenize input
        inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512)
        
        # Move to same device as model
        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # Generate translation
        with torch.no_grad():
            generated_tokens = model.generate(
                **inputs,
                forced_bos_token_id=tokenizer.get_lang_id(tgt_code),
                max_length=512,
                num_beams=5,
                early_stopping=True
            )
        
        # Decode translation
        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)[0]
        
        logger.info(f"M2M100 translation: {source_lang} -> {target_lang} successful")
        return translated_text
        
    except Exception as e:
        logger.error(f"M2M100 translation failed: {e}")
        return None

def translate_text(
    text: str, 
    source_lang: str, 
    target_lang: str,
    method: str = 'auto'
) -> Tuple[Optional[str], str]:
    """
    Translate text from source language to target language.
    
    Args:
        text: Text to translate
        source_lang: Source language (name or code)
        target_lang: Target language (name or code)
        method: Translation method ('auto', 'google', 'm2m100', 'marian')
        
    Returns:
        Tuple of (translated_text, method_used)
    """
    if not text or not text.strip():
        return text, 'no_translation_needed'
    
    # Normalize language names
    source_lang = source_lang.lower()
    target_lang = target_lang.lower()
    
    # Check if translation is needed
    src_code = LANGUAGE_MAPPINGS.get(source_lang, {}).get('code', source_lang)
    tgt_code = LANGUAGE_MAPPINGS.get(target_lang, {}).get('code', target_lang)
    
    if src_code == tgt_code:
        logger.info(f"No translation needed: {source_lang} == {target_lang}")
        return text, 'no_translation_needed'
    
    # Try translation methods in order of preference
    methods_to_try = [method] if method != 'auto' else TRANSLATION_METHODS
    
    for translation_method in methods_to_try:
        if translation_method == 'google_translate':
            result = translate_with_google(text, source_lang, target_lang)
            if result:
                return result, 'google_translate'
                
        elif translation_method == 'm2m100':
            result = translate_with_m2m100(text, source_lang, target_lang)
            if result:
                return result, 'm2m100'
                
        elif translation_method == 'fallback':
            # Simple fallback - return original text with warning
            logger.warning(f"All translation methods failed, returning original text")
            return text, 'fallback'
    
    # If all methods fail, return original text
    logger.error(f"Translation failed for {source_lang} -> {target_lang}")
    return text, 'failed'

from concurrent.futures import ThreadPoolExecutor, as_completed

def translate_segments(
    segments: List[Dict], 
    source_lang: str, 
    target_lang: str,
    method: str = 'auto',
    max_workers: int = 5   # number of parallel threads
) -> List[Dict]:
    """
    Translate text segments from source to target language with multithreading.

    Args:
        segments: List of segment dictionaries with 'text' field
        source_lang: Source language
        target_lang: Target language
        method: Translation method to use
        max_workers: Number of threads for parallel translation

    Returns:
        List of segments with translated text
    """
    if not segments:
        return segments
    
    logger.info(f"Translating {len(segments)} segments from {source_lang} to {target_lang}")

    translated_segments = [None] * len(segments)  # placeholder for results
    translation_stats = {'success': 0, 'failed': 0, 'methods': {}}

    def translate_one(index, segment):
        original_text = segment.get('text', '')
        if not original_text.strip():
            return index, segment.copy()

        translated_text, method_used = translate_text(
            original_text, source_lang, target_lang, method
        )

        # stats
        if method_used != 'failed':
            translation_stats['success'] += 1
            translation_stats['methods'][method_used] = translation_stats['methods'].get(method_used, 0) + 1
        else:
            translation_stats['failed'] += 1

        new_segment = segment.copy()
        new_segment['text'] = translated_text
        new_segment['original_text'] = original_text
        new_segment['translation_method'] = method_used

        return index, new_segment

    # Run translations in parallel
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(translate_one, i, seg) for i, seg in enumerate(segments)]
        for future in as_completed(futures):
            idx, translated_seg = future.result()
            translated_segments[idx] = translated_seg

    logger.info(f"Translation complete: {translation_stats['success']} successful, {translation_stats['failed']} failed")
    logger.info(f"Methods used: {translation_stats['methods']}")
    
    return translated_segments


# def translate_segments(
#     segments: List[Dict], 
#     source_lang: str, 
#     target_lang: str,
#     method: str = 'auto'
# ) -> List[Dict]:
#     """
#     Translate text segments from source to target language.
    
#     Args:
#         segments: List of segment dictionaries with 'text' field
#         source_lang: Source language
#         target_lang: Target language
#         method: Translation method to use
        
#     Returns:
#         List of segments with translated text
#     """
#     if not segments:
#         return segments
    
#     logger.info(f"Translating {len(segments)} segments from {source_lang} to {target_lang}")
    
#     translated_segments = []
#     translation_stats = {'success': 0, 'failed': 0, 'methods': {}}
    
#     for i, segment in enumerate(segments):
#         original_text = segment.get('text', '')
        
#         if not original_text.strip():
#             translated_segments.append(segment.copy())
#             continue
        
#         # Translate the text
#         translated_text, method_used = translate_text(
#             original_text, source_lang, target_lang, method
#         )
        
#         # Update statistics
#         if method_used not in ['failed']:
#             translation_stats['success'] += 1
#             translation_stats['methods'][method_used] = translation_stats['methods'].get(method_used, 0) + 1
#         else:
#             translation_stats['failed'] += 1
        
#         # Create new segment with translated text
#         new_segment = segment.copy()
#         new_segment['text'] = translated_text
#         new_segment['original_text'] = original_text
#         new_segment['translation_method'] = method_used
        
#         translated_segments.append(new_segment)
        
#         # Progress logging
#         if (i + 1) % 10 == 0:
#             logger.info(f"Translated {i + 1}/{len(segments)} segments")
    
#     # Log final statistics
#     logger.info(f"Translation complete: {translation_stats['success']} successful, {translation_stats['failed']} failed")
#     logger.info(f"Methods used: {translation_stats['methods']}")
    
#     return translated_segments

def get_available_languages() -> Dict[str, Dict]:
    """
    Get all available languages with their metadata.
    
    Returns:
        Dictionary mapping language names to their metadata
    """
    return LANGUAGE_MAPPINGS.copy()

def get_language_pairs() -> List[Tuple[str, str]]:
    """
    Get all possible language pairs for translation.
    
    Returns:
        List of (source, target) language pairs
    """
    languages = list(LANGUAGE_MAPPINGS.keys())
    pairs = []
    
    for source in languages:
        for target in languages:
            if source != target:
                pairs.append((source, target))
    
    return pairs

def detect_language(text: str) -> Optional[str]:
    """
    Detect the language of given text.
    
    Args:
        text: Text to analyze
        
    Returns:
        Detected language code or None if detection fails
    """
    try:
        translator = get_google_translator()
        if not translator:
            return None
        
        detection = translator.detect(text)
        detected_lang = detection.lang
        
        # Convert Google language code to our internal format
        for lang_name, lang_data in LANGUAGE_MAPPINGS.items():
            if lang_data['google'] == detected_lang:
                return lang_name
        
        return detected_lang
        
    except Exception as e:
        logger.error(f"Language detection failed: {e}")
        return None

# Cache management functions
def save_translation_cache(filepath: str = None):
    """Save translation cache to disk."""
    if not filepath:
        cache_dir = ensure_cache_dir()
        filepath = cache_dir / 'translation_cache.pkl'
    
    try:
        cache_data = {
            'language_mappings': LANGUAGE_MAPPINGS,
            'timestamp': time.time()
        }
        
        with open(filepath, 'wb') as f:
            pickle.dump(cache_data, f)
        
        logger.info(f"Translation cache saved to {filepath}")
    except Exception as e:
        logger.error(f"Failed to save translation cache: {e}")

def load_translation_cache(filepath: str = None):
    """Load translation cache from disk."""
    if not filepath:
        cache_dir = ensure_cache_dir()
        filepath = cache_dir / 'translation_cache.pkl'
    
    try:
        if Path(filepath).exists():
            with open(filepath, 'rb') as f:
                cache_data = pickle.load(f)
            
            logger.info(f"Translation cache loaded from {filepath}")
            return cache_data
        else:
            logger.info("No translation cache found")
            return None
    except Exception as e:
        logger.error(f"Failed to load translation cache: {e}")
        return None

# Initialize cache on module import
ensure_cache_dir()
</file>

<file path=".gitignore">
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
venv/
ENV/
# Environment variables
.env

# IDE
.vscode/
.idea/
*.swp
*.swo
*~

# Project specific
uploads/*
!uploads/.gitkeep
processed/*
!processed/.gitkeep

# Logs
*.log

# Large files
*.mp4
*.mp3
*.wav
*.avi
*.mov
*.mkv

# OS generated files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db
downloads/
zzz.py
wsgi.py
voice-cloning/
</file>

</files>
